\documentclass[manuscript,review]{acmart}

\let\Bbbk\undefined % Undefines some symbols for amssymb (e.g. so I can use \blacklozenge)
\usepackage{amssymb}
\usepackage[boxed, vlined, linesnumbered, resetcount]{algorithm2e} 
\usepackage{bm}
\usepackage{float}
\usepackage{listings}
\usepackage{multirow}
\usepackage{physics}
\usepackage{subfigure}
\usepackage[binary-units=true]{siunitx}
\usepackage{todonotes}

\usepackage{hyperref} % Should usually be loaded last. 

\newfloat{lstfloat}{htbp}{lop} % To put the listings in. 
\renewcommand{\lstlistingname}{Code} % Rename them as codes. 


\SetAlCapSty{} % Removes bold in caption for consistency. 
\SetAlCapSkip{0.5em} % Increase algorithm spacing between box and caption.
\SetAlCapNameFnt{\small\sffamily}
\SetAlCapFnt{\small\sffamily}
\SetAlgoCaptionSeparator{.}

% For formatting the C code. 
\lstdefinestyle{C}{
    language=C,
    basicstyle=\small\ttfamily,
    keywordstyle=\small\ttfamily,
    morekeywords={omp,simd,reduction,simdlen,declare,inline,bool,restrict,half},
    otherkeywords={\#pragma,\_\_fp16},
    frame = single,
    captionpos=b,
}


\DeclareMathOperator*{\argmin}{argmin} % For argmin. 

\citestyle{acmnumeric} % For the numeric citation style. 

\title{Approximating inverse cumulative distribution functions to produce approximate random variables}

\author{Michael Giles}
\email{mike.giles@maths.ox.ac.uk}

\author{Oliver Sheridan-Methven}
\email{oliver.sheridan-methven@hotmail.co.uk}

\affiliation{%
\institution{Mathematical Institute, Oxford University}
\city{Oxford}
\country{UK}}

\keywords{approximations, random variables, inverse cumulative distribution functions, the Gaussian distribution, the non central $ \chi^2 $ distribution, multilevel Monte Carlo, the Euler-Maruyama scheme,  and high performance computing.}

\begin{document}

\begin{abstract}
For random variables produced through the inverse transform method, approximate random variables are introduced, which are produced by approximations to a distribution's inverse cumulative distribution function. These approximations are designed to be computationally inexpensive, and much cheaper than exact library functions, and thus highly suitable for use in Monte Carlo simulations. Two approximations are presented for the Gaussian distribution: a piecewise constant on equally spaced intervals, and a piecewise linear using geometrically decaying interval dense at the distribution's tails. The convergence of the approximations are demonstrated, and the computational savings measured for C implementations. Implementations tailored for Intel and Arm hardwares are inspected, alongside hardware agnostic implementations built using OpenMP. The savings are incorporated into a nested multilevel Monte Carlo framework with the Euler-Maruyama scheme to exploit the speed ups without losing accuracy. These ideas are empirically extended to the Milstein scheme, and the Cox-Ingersoll-Ross process' non central $ \chi^2 $ distribution. 
\end{abstract}

% cf. https://dl.acm.org/ccs#
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002950.10003714.10003715</concept_id>
       <concept_desc>Mathematics of computing~Numerical analysis</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003705.10011686</concept_id>
       <concept_desc>Mathematics of computing~Mathematical software performance</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003705.10003708</concept_id>
       <concept_desc>Mathematics of computing~Statistical software</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003714.10003736.10003737</concept_id>
       <concept_desc>Mathematics of computing~Approximation</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003741.10003746</concept_id>
       <concept_desc>Mathematics of computing~Continuous functions</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010169.10010170.10010173</concept_id>
       <concept_desc>Computing methodologies~Vector / streaming algorithms</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010341.10010342.10010345</concept_id>
       <concept_desc>Computing methodologies~Uncertainty quantification</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010341.10010349.10010345</concept_id>
       <concept_desc>Computing methodologies~Uncertainty quantification</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010341.10010349.10010362</concept_id>
       <concept_desc>Computing methodologies~Massively parallel and high-performance simulations</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Numerical analysis}
\ccsdesc[300]{Mathematics of computing~Mathematical software performance}
\ccsdesc[500]{Mathematics of computing~Statistical software}
\ccsdesc[100]{Mathematics of computing~Approximation}
\ccsdesc[100]{Mathematics of computing~Continuous functions}
\ccsdesc[100]{Computing methodologies~Vector / streaming algorithms}
\ccsdesc[300]{Computing methodologies~Uncertainty quantification}
\ccsdesc[300]{Computing methodologies~Uncertainty quantification}
\ccsdesc[300]{Computing methodologies~Massively parallel and high-performance simulations}

\maketitle

\section{Introduction}
\label{sec:introduction}

There are a wide range of computational tasks which centre on using random numbers, including: encryption, animation rendering \citep{lee2017vectorized}, and financial simulations \citep{glasserman2013monte}, to name a few. A frequent bottleneck to these is the generation of random numbers, whether these are cryptographically secure random numbers for encryption, or random numbers from a specific statistical distribution such as in Monte Carlo simulations.  While computers have many excellent and fast implementations of random number generators for the uniform distribution, sampling random variables from a generic distribution is often computationally very expensive. (For certain distributions there are some specific transformations, such as the Box-Muller scheme \citep{box1958note} for the Gaussian distribution). 

We consider random variables produced using the inverse transform method \citep{glasserman2013monte}, which enables sampling from any distribution, and thus is very widely applicable. Additionally, it is crucial for quasi-Monte Carlo simulations \citep{giles2009multilevel_qmc,lecuyer2016randomized} so no samples are rejected and the low-discrepancy property is preserved \citep{tezuka1995uniform}, where such applications in financial simulations are common \citep{joy1996quasi,xu2015high}. Furthermore, we demonstrate how the inverse transform method is particularly well suited to analysis, and that it naturally provides a coupling mechanism for multilevel Monte Carlo applications for a range of stochastic processes, statistical distributions, and numerical schemes. 

Our analysis focuses on the Gaussian distribution (a.k.a.\ the Normal distribution), and the motivations are threefold. Firstly, the distribution is representative of several continuous distributions, and due to the central limit theorem is often the limiting case. Secondly, it is analytically tractable and often admits exact results or is amenable to approximation. Lastly, it is ubiquitous in both academic analysis and scientific computation, with its role cemented within It\^{o} calculus and financial simulations. 

To produce Gaussian random variables will require the Gaussian distribution's inverse cumulative distribution function. Constructing approximations accurate to machine precision has long been under the attention of the scientific community \citep{hastings1955approximations,evans1974algorithm70,beasley1985percentage,wichura1988algorithm,marsaglia1994rapid,giles2011approximating}, where the \textit{de facto} routine implemented in most libraries is by \citet{wichura1988algorithm}. While some applications require such accurate approximations, such as in evaluating $ p $-values in various statistical applications, for numerous others such accuracy is superfluous and unnecessarily costly, as is the case in Monte Carlo simulations. 

To alleviate the cost of exactly sampling from the Gaussian distribution, the most circumventing is to substitute these samples with random variables with similar statistics. The bulk of such schemes follow a moment matching procedure, where the most well known is to use Rademacher random variables (which take the values $ \pm 1 $ with equal probability \citep[page~XXXII]{kloeden1999numerical}, giving rise to the weak Euler-Maruyama scheme), matching the mean. Another is to sum twelve uniform random variables and subtract the mean \citep[page~500]{munk2011fixed}, which matches the mean and variance, and is still computationally cheap. The most recent work in this direction is by \citet{muller2015improving}, who produce either a three or four point distribution, where the probability mass is positioned so the resulting distribution's moments match the lowest few moments of the Gaussian distribution. 

The direction we follow is closer aligned to the work by \citet{giles2019random_quadrature,giles2019random_multilevel}, whose analysis proposes a cost model for producing the individual random bits constituting a uniform random number. They truncate their uniforms to a fixed number of bits and then add a small offset before using the inverse transform method. The nett result from this is to produce a piecewise constant approximation, where the intervals are all of equal width, and the values are the mid-point values of the respective intervals. 

The work we present directly looks to substitute random variables produced from the inverse transform method using the exact inverse cumulative distribution function, with those produced using an approximation to the inverse cumulative distribution function. While this is primarily motivated by computational savings, our framework encompasses and recovers the distributions produced from the various moment matching schemes and the truncated bit schemes. 

Having a framework capable of producing various such distributions has several benefits. The first is that by refining our approximation, we can construct distributions resembling the exact distribution to an arbitrary fidelity. This allows for a trade-off between computational savings, and a lower degree of variance between the exact distribution and its approximation. This naturally  introduces two tiers of simulations: those using a cheap but approximate distribution, and those using an expensive but exact distribution. This immediately facilitates the multilevel Monte Carlo setting by \citet{giles2008multilevel}, where fidelity and cost are balanced to obtain the minimal computational time. As an example, Rademacher random variables, while very cheap, are too crude to fully exploit the savings possible with multilevel Monte Carlo. However, the fidelity of our approximations can be adjusted to fully exploit the possible savings. 

The second benefit of our approach is that while the approximations are  specified mathematically, their implementations are left unspecified. This flexibility facilitates constructing approximations which can be tailored to a specific hardware or architecture. We will present two approximations, whose implementations can gain speed by transitioning the work load from primarily using the floating-point processing units to instead exploiting the cache hierarchy. Further to this, our approximations are designed with vector hardware in mind, and are non-branching, and thus suitable for implementation using single-instruction multiple data (SIMD) instructions, (including Arm's new scalable vector extension (SVE) instruction set). Furthermore, for hardware with very large vectors, such as the \SI{512}{\bit} wide vectors on Intel's AVX-512 and Fujitsu's Arm-based A64FX (such as those in the new Fugaku supercomputer), we demonstrate implementations unrivalled in their computational performance on the latest hardwares (albeit not vector length agnostic). Previous work using low precision bit wise approximations targetted at reconfigurable field programmable gate arrays has previously motivated the related works by \citet{brugger2014mixed} and \citet{omland2015exploiting}.

We primarily focus our attention on the analysis and implementation of two approximations: a piecewise constant approximation using equally sized intervals, and a piecewise linear approximation using geometrically decaying intervals. The former is an extension of the work by \citet[theorem~1]{giles2019random_quadrature} to higher moments, while the latter is a novel analysis, capable of both the highest speeds and fidelities. Although we will demonstrate how these are incorporated into a multilevel Monte Carlo framework, the subsequent analysis is performed by \citeauthor{giles2020approximate} \citep{giles2020approximate,sheridan2020nested} and omitted from this work. We will however showcase the savings a practitioner adopting our framework can expect to achieve from even a minimal incorporation of our proposals. 

Section~\ref{sec:approximate_gaussian_random_variables} introduces and analyses our two approximations, providing bounds on the error of the approximations. Section~\ref{sec:high_performance_impementations} discusses the production of high performance implementations, the code for which is collected into a central repository maintained by \citet{sheridan2020approximate_random,sheridan2020approximate_inverse}. Section~\ref{sec:multilevel_monte_carlo} introduces multilevel Monte Carlo as a natural application for our approximations, and demonstrates the possible computational savings. Section~\ref{sec:the_non_central_chi_squared_distribution} extends our approximations to the non central $ \chi^2 $ distribution, representing a very expensive parametrised distribution which arises in simulations of the Cox-Ingersoll-Ross process. Lastly, section~\ref{sec:conclusions} presents the conclusions from this work. 

\section{Approximate Gaussian random variables}
\label{sec:approximate_gaussian_random_variables}

We will be using the inverse transform method \citep[2.2.1]{glasserman2013monte} to produce random variable samples from a desired distribution. The key step to this method is evaluating a distribution's inverse cumulative distribution function (sometimes called the percentile or percent point functions). We will focus on sampling from the Gaussian distribution, whose inverse cumulative distribution function we denote by $ \Phi^{-1} \colon (0, 1) \to \mathbb{R} $ (some authors use $ N^{-1} $), and similarly whose cumulative distribution function and probability density function we denoted by $ \Phi $ and $ \phi $ respectively. 

Our key proposal is to use the inverse transform method with an approximation to the inverse cumulative distribution function. As the resulting distribution will not exactly match the desired distribution, we call random variables produced in this way \emph{approximate random variables}, (and those without the approximation as \emph{exact random variables} for added clarity). In general, we will denote exact Gaussian random variables by $ Z $ and approximate Gaussian random variables by $ \tilde{Z} $. The key motivation for introducing approximate random variables is that they are computationally cheaper to generate than exact random variables. Consequently, our key motivating criteria in forming approximations will be a simple mathematical construction, hoping this fosters fast implementations. As such there is a slight trade off between simplicity and fidelity, where we will primarily be targetting the simplicity. 

In this section, we present two approximation types: a piecewise constant, and a piecewise linear. For both we will bound the $ L^p $ error, focusing on their mathematical constructions and analyses. Their implementations and utilisation will be detailed in sections~\ref{sec:high_performance_impementations} and \ref{sec:multilevel_monte_carlo}. Both analyses will share and frequently use approximations for moments and tail values of the Gaussian distribution, which we gather together in section~\ref{sec:approximating_tail_values_and_high_order_moments}. Thereafter, the piecewise constant and linear approximations are analysed in sections~\ref{sec:piecewise_constant_approximations_on_equal_intervals} and \ref{sec:piecewise_linear_approximations_on_geometric_intervals}.

\subsection{Approximating tail values and high order moments}
\label{sec:approximating_tail_values_and_high_order_moments}

For the Gaussian distribution with probability density function $ \phi(z) \coloneqq \tfrac{1}{\sqrt{2\pi}} {\exp}(-\tfrac{1}{2}z^2) $, we will require approximations for tail values and high order moments. The approximations will usually become more accurate for more extreme tail values, and so we use the notation by \citet{giles2019random_quadrature} that $ f(z) \approx g(z) $ denotes $ \lim_{z\to\infty} \tfrac{f(z)}{g(z)} = 1 $, and similarly $ f(z) \lessapprox g(z) $ to denote $ f(z) \leq g(z) $ and $ f(z) \approx g(z) $. Our key results will be lemmas~\ref{lemma:approximate_tail_values} and \ref{lemma:approximate_moments} which will bound the tail values and high order moments. Lemma~\ref{lemma:approximate_tail_values} is an extension on a similar result by \citet[lemma~7]{giles2019random_quadrature}, extended to give enclosing bounds. Similarly, lemma~\ref{lemma:approximate_moments} is partly an extension of a result by \citet[lemma~9]{giles2019random_quadrature}, but extended to arbitrarily high moments rather than just the second. As such, neither of these lemmas are particularly noteworthy in themselves and their proofs resemble work by \citet[appendix~A]{giles2019random_quadrature}. Similarly, our resulting error bounds for the piecewise constant approximation in section~\ref{sec:piecewise_constant_approximations_on_equal_intervals} will closely resemble a related result by \citet[theorem~1]{giles2019random_quadrature}. However, our main result for the piecewise linear approximation in section~\ref{sec:piecewise_linear_approximations_on_geometric_intervals} is novel and will require these results, and thus we include them here primarily for completeness. 

\begin{lemma}
\label{lemma:approximate_tail_values}
Defining $ z_q \coloneqq \Phi^{-1}(1 - 2^{-q})$, then for $ q \gg 1 $ we have the bounds $ 1 \lessapprox \tfrac{2^q\phi(z_q)}{z_q} \lessapprox (1 - z_q^{-2})^{-1}$  and $ \sqrt{q \log(4) - \log(q \pi \log(16))} \lessapprox z_q  \lessapprox \sqrt{q \log(4)}$.
\end{lemma}

\begin{proof}
As $ 2^{-q} = 1 - \Phi(z_q) $, it remains to evaluate the integral $ 1 - \Phi(z_q) = \int_{z_q}^{\infty} \phi(s) \dd{s} $. Using $ \phi'(z) =  -z \phi(z) $ and integrating by parts gives $ \int_{z_q}^{\infty} \phi(s) \dd{s} = \eval*[\tfrac{-\phi(s)}{s}|_{z_q}^{\infty} - \int_{z_q}^{\infty} \tfrac{1}{s^2} \phi(s) \dd{s} $, and repeating this gives $ \int_{z_q}^{\infty} \phi(s) \dd{s}  = \phi(z_q)(\tfrac{1}{z_q} - \tfrac{1}{z_q^3} + \cdots) $. To first order we have $ 2^{-q} \lessapprox \tfrac{\phi(z_q)}{z_q} $ and to second order $ 2^{-q} \gtrapprox \phi(z_q)(\tfrac{1}{z_q} - \tfrac{1}{z_q^3}) $.  Substituting $ \phi(z) \coloneqq \tfrac{1}{\sqrt{2 \pi}} {\exp}(\tfrac{-z^2}{2}) $ into $ 2^{-q} \lessapprox \tfrac{\phi(z_q)}{z_q} $, this re-arranges to $ z_q \approx \sqrt{q{\log}(4) - 2{\log}(\smash{z_q}) - {\log}(2\pi)} $ which gives $ z_q \lessapprox \sqrt{q \log(4)} $. Substituting this upper bound for $ z_q $ recursively into the approximation  for $ z_q $ gives $ z_q \gtrapprox  \sqrt{q \log(4) - \log(q \pi \log(16))}$. \qedhere
\end{proof}

\begin{lemma}
\label{lemma:approximate_moments}
For integers $ p \geq 2 $ we have $ \int_{0}^{z} \phi(s)^{1-p} \dd{s} \approx \tfrac{\phi(z)^{1-p}}{(p-1)z}  $ and $ \int_{z}^{\infty} (s-z)^p\phi(s) \dd{s} \approx \tfrac{p!\phi(z)}{z^{p+1}} $.
\end{lemma}

\begin{proof}
The relation  $ \int_{0}^{z} \phi(s)^{1-p} \dd{s} \approx \tfrac{\phi(z)^{1-p}}{(p-1)z}  $ is demonstrated by applying L'H\^{o}pital's rule
\begin{equation*}
\lim_{z \to \infty} \dfrac{\int_{0}^{z} \phi(s)^{1-p} \dd{s}}{\left(\dfrac{\phi(z)^{1-p}}{(p-1)z}\right)} 
=  \lim_{z \to \infty} \dfrac{\phi(z)^{1-p}}{\phi(z)^{1-p}\left(1 - \dfrac{1}{(p-1)z^2} \right)} 
= \lim_{z \to \infty} \cfrac{1}{\left(1 - \dfrac{1}{(p-1)z^2}\right)} 
= 1.
\end{equation*}

However, the second integral $ \int_{z}^{\infty} (s-z)^p\phi(s) \dd{s} $ proceeds by integrating by parts. We differentiate $ (s-z)^p $ to give $ \dv{s} (s-z)^p = p (s-z)^{p-1} $ and integrate $ \phi(s) $, where we recall that $ \int_{z}^{\infty} \phi(s) \dd{s}  = \phi(z)(\tfrac{1}{z} - \tfrac{1}{z^3} + \cdots) $ and hence $ \int_{-\infty}^{z} \phi(s) \dd{s}  = 1 -\phi(z)(\tfrac{1}{z} - \tfrac{1}{z^3} + \cdots) $, hence giving $ {\dv{s}} (\phi(s)(\tfrac{1}{s} - \tfrac{1}{s^3} + \cdots))  = -\phi(s) $. Integration by parts gives
\begin{equation*}
\int_{z}^{\infty} (s-z)^p\phi(s) \dd{s} \lessapprox \underbrace{\eval[p(s-z)^{p-1}\phi(s)\left(-\dfrac{1}{s} + \dfrac{1}{s^3} - \cdots\right)|_{z}^{\infty}}_{{} = 0} {} + p \int_{z}^{\infty} (s-z)^{p-1} \phi(s) \left(\dfrac{1}{s} - \dfrac{1}{s^3} + \cdots\right) \dd{s}.
\end{equation*}
Upper bounding $ (\tfrac{1}{s} - \tfrac{1}{s^3} + \cdots )$ by $ \tfrac{1}{z} $ and iteratively evaluating the integral gives
\begin{equation*}
\int_{z}^{\infty} (s-z)^p\phi(s) \dd{s}
\lessapprox \dfrac{p}{z} \int_{z}^{\infty} (s-z)^{p-1} \phi(s) \dd{s} 
\lessapprox \dfrac{p!}{z^p} \int_{z}^{\infty} \phi(s) \dd{s} 
\lessapprox \dfrac{p!}{z^p} \left(\dfrac{\phi(z)}{z}\right) 
\lessapprox \frac{p!\phi(z)}{z^{p+1}}. \qedhere
\end{equation*}
\end{proof}

\subsection{Piecewise constant approximations on equal intervals}
\label{sec:piecewise_constant_approximations_on_equal_intervals}

Mathematically it is straightforward motivate a piecewise constant approximation as the simplest possible approximation to use, especially using equally spaced intervals. As the range of values will go from a continuous to a discrete set, we say the distribution has become \emph{quantised} and denote our approximation as $ Q \approx \Phi^{-1} $, where for a uniform random variable $ U \sim \mathcal{U}(0, 1)$ we have $ Z \coloneqq \Phi^{-1}(U) $ and $ \tilde{Z} \coloneqq Q(U) $. A preview of such an approximation is shown in figure~\ref{fig:piecewise_constant_gaussian_approximation}, where the error will be measured using the $ L^p $ norm $ \lVert f \rVert_p \coloneqq (\int_0^1 \abs{f(u)}^p \dd{u})^{1/p} $.

\begin{figure}[htb]
\centering

\hfill
\subfigure[An example approximation using 8 intervals.\label{fig:piecewise_constant_gaussian_approximation}]{\includegraphics{piecewise_constant_gaussian_approximation}} \hfill 
\subfigure[The $ L^p $ error and the bound from theorem~\ref{thm:piecewise_constant_approximation_error}.\label{fig:piecewise_constant_gaussian_approximation_error}]{\includegraphics{piecewise_constant_gaussian_approximation_error}}\hfill

\caption{The piecewise constant approximation $ \tilde{Z}_k^{L^1} $ from corollary~\ref{cor:piecewise_constant_constructions} with equally spaced intervals, and its error.}
\label{fig:piecewise_constant_approximation}
\Description[The piecewise constant approximation is of a reasonable fidelity and its error decreases with more intervals.]{The piecewise constant approximation with 8 intervals fits with a modest fidelity the exact inverse Gaussian cumulative distribution function. We consider the error in various norms, which each show the error decreases as the number of intervals in increased, and this decrease is well described by our analytic bound.}
\end{figure}

Before presenting theorem~\ref{thm:piecewise_constant_approximation_error}, we can briefly comment on the error seen in figure~\ref{fig:piecewise_constant_gaussian_approximation_error}. Specifically looking at the root mean squared error (RMSE), corresponding to the $ L^2 $ norm, we can see that increasing the number of intervals from 2 to $ 10^3 $ gives a drop of $ 10^2 $ in the RMSE. For our multilevel Monte Carlo applications later, having approximately 1000 intervals gives a very good fidelity, whereas the lower fidelity approximations with much higher errors, like the Rademacher random variables resembling the distribution with 2 intervals, has a much lower fidelity. Being able to achieve a reasonable fidelity from our approximation ensures that we achieve the largest portion of the possible temporal savings offered from our approximations. 

As we have already mentioned, the piecewise constant approximation is closely related to the resulting approximation produced by \citet{giles2019random_quadrature}, whose approximation arises from considering uniform random variables truncated to a finite number of bits of precision. Thus our main result from this section, theorem~\ref{thm:piecewise_constant_approximation_error}, closely resembles a related result by \citet[theorem~1]{giles2019random_quadrature}. To put our extension into context, we paraphrase the similar result from \citet{giles2019random_quadrature}, which is that for a piecewise constant approximation using $ 2^q $ intervals, for some integer $ q \gg 1 $, then for constant values equal to each interval's midpoint value they have $ \lVert Z - \tilde{Z}\rVert_2^2 = O(2^{-q}q^{-1}) $. Our result from theorem~\ref{thm:piecewise_constant_approximation_error} extends this to $ \lVert Z - \tilde{Z}\rVert_p^p = O(2^{-q} q^{-p/2}) $ for $ p \geq 2 $, and numerous other possible constant values other than the midpoint's. Our result enables us to increase the order of the error to arbitrarily high norms is interesting in its own right, as it shows that as the intervals become increasing small (corresponding to $ q \to \infty $), the dominant term effecting the $ L^p $ error is the geometric decay $ O(2^{-2/p}) $, and thus the convergence exists but is slower in higher norms, with the polynomial $ O(q^{-1/2}) $ term being comparatively negligible (as we can see in figure~\ref{fig:piecewise_constant_gaussian_approximation_error}). Additionally, in the related analysis incorporating approximate random variables into a nested multilevel Monte Carlo framework by \citeauthor{giles2020approximate} \citep{giles2020approximate,sheridan2020nested}, their bounds on the variance of the multilevel Monte Carlo correction term (discussed more in section~\ref{sec:multilevel_monte_carlo}) rely on the existence of the $ L^p $ error for $ p > 2 $. Hence, while this strengthening of the result may appear only slight, it is crucial for nested multilevel Monte Carlo. 

We can now present our main result concerning piecewise constant approximations, namely theorem~\ref{thm:piecewise_constant_approximation_error}. In this we will leave the interval values largely unspecified, and later demonstrate in corollary~\ref{cor:piecewise_constant_constructions} several choices for the midpoint fit within the scope of theorem~\ref{thm:piecewise_constant_approximation_error}.

\begin{theorem}
\label{thm:piecewise_constant_approximation_error}
Let a piecewise constant approximation $ Q \approx \Phi^{-1} $ use $ 2^q $ equally spaced intervals for some integer $ q > 1 $. Denote the intervals $ I_k \coloneqq (k2^{-q}, (k+1)2^{-q}) \equiv (u_k, u_{k+1}) $ for $ k \in \{0, 1, 2, \ldots, K\} $ where $ K \equiv 2^q - 1 $. On each interval the approximation constant is $ Q_k \coloneqq Q(u) $ for $ u \in I_k $. We assume there exists a constant $ C $ independent of $ q $ such that:
\begin{enumerate}
\item \label{con:symmetry} $ Q_k = -Q_{K-k} $ for $ k \in \{0,1,2,\ldots,K\} $. 
\item \label{con:intermediate_values} $ \Phi^{-1}(u_k) \leq Q_k \leq \Phi^{-1}(u_{k+1}) $ for $ k \in \{1,2, \ldots, K-1\} $.
\item \label{con:bounded_asymptotic_growth} $ \Phi^{-1}(u_K) \leq Q_K \leq \Phi^{-1}(u_K) + Cq^{-1/2} $.
\end{enumerate}
Then for any even integer $ p \geq 2 $ we have $ \lVert  Q - \Phi^{-1} \rVert_p^p = O(2^{-q}q^{-p/2})$ for $ q \gg 1 $.
\end{theorem}

\begin{proof} Defining $ A_k \coloneqq \int_{u_k}^{u_{k+1}} \lvert\Phi^{-1}(u) - Q_k\rvert^p \dd{u} $ for $ k \in \{0,1, \ldots, K\} $, then we have $ \lVert Q - \Phi^{-1} \rVert_p^p = \sum_{k=0}^{K} A_k $.  We use condition (\ref{con:symmetry}) to reduce our considerations to the domain $ (\tfrac{1}{2}, 1) $, where $ \Phi^{-1} $ is strictly positive and convex, and thus obtain $ \lVert Q - \Phi^{-1} \rVert_p^p = 2 \sum_{k=2^{q-1}}^{K-1} A_k + 2 A_K $. As $ \Phi^{-1} $ is convex, then from the intermediate value theorem there exists a $ \xi_k \in I_k $ such that $ Q_k = \Phi^{-1}(\xi_k) $. Noting that $ \dv{z} \Phi^{-1}(z) = \tfrac{1}{\phi(\Phi^{-1}(z))} $, then from the mean value theorem  for any $ u \in [u_k, \xi_k)$ there exists an $ \eta_k \in [u_k, \xi_k)$ such that $ \Phi^{-1}(u) - \Phi^{-1}(\xi_k) = \tfrac{u - \xi_k}{\phi(\Phi^{-1}(\eta_k))} $. Furthermore, as $ \phi $ is monotonically decreasing in $ (\tfrac{1}{2}, 1) $, then introducing $ z_k \coloneqq \Phi^{-1}(u_k) $ we have $ \lvert \Phi^{-1}(u) - \Phi^{-1}(\xi_k)\rvert \leq \tfrac{2^{-q}}{\phi(z_{k+1})}$.  Using this to bound $ \sum_{k=2^{q-1}}^{K-1} A_k $ in our expression for $ \lVert Q - \Phi^{-1}\rVert_p^p $ gives
\begin{equation*}
\sum_{k=2^{q-1}}^{K-1} A_k 
\leq \sum_{k=2^{q-1}}^{K - 1} \left(\frac{2^{-q}}{\phi(z_{k+1})}\right)^p 2^{-q} 
\leq 2^{-qp - q} \sum_{k=2^{q-1}}^{K - 1}  \phi(z_{k+1})^{-p} 
\leq 2^{-pq} \int_{\frac{1}{2}}^{1 - 2^{-q}} \phi(\Phi^{-1}(u))^{-p} \dd{u} + 2^{-q(p+1)}\phi(z_K)^{-p},
\end{equation*}
where the last bound comes from considering a translated integral. Changing integration variables the integral becomes $  \int_{0}^{z_K} \phi(z)^{1-p} \dd{z} $, from which we can use lemma~\ref{lemma:approximate_moments} to give 
\begin{equation*}
\sum_{k=2^{q-1}}^{K-1} A_k 
\leq \frac{2^{-q} z_K^{-p}}{p-1} \left(\frac{2^q\phi(z_K)}{z_K}\right)^{1-p} + 2^{-q}z_K^{-p} \left(\frac{2^q\phi(z_K)}{z_K}\right)^{-p}
\leq 2^{-q} \left(\frac{p}{p-1}\right) z_K^{-p},
\end{equation*}
where the last bound follows from lemma~\ref{lemma:approximate_tail_values}. Turning our attention to the final interval's contribution $ A_K $, then using Jensen's inequality, condition~(\ref{con:bounded_asymptotic_growth}), and lemmas~\ref{lemma:approximate_tail_values} and \ref{lemma:approximate_moments} we obtain
\begin{equation*}
A_K 
\leq 2^{p-1} \int_{z_K}^{\infty} \lvert Q_K - z\rvert^p \phi(z) \dd{z} +  2^{p-1} \int_{u_K}^{1} \lvert Q_K - z_K \rvert^p \dd{u} 
\leq 2^{p-q-1} p! z_K^{-p} + 2^{p-q-1} C^p q^{-p/2}.
\end{equation*}
Combining our two bound for $ \sum_{k=2^{q-1}}^{K-1} A_k $ and $ A_K $ into our expression for  $ \lVert Q - \Phi^{-1}\rVert_p^p $ we obtain
\begin{equation*}
\lVert Q - \Phi^{-1}\rVert_p^p 
\leq 2^{-q+1} \left(\frac{p}{p-1}\right) z_K^{-p} + 2^{p-q} p! z_K^{-p} + 2^{p-q} C^p q^{-p/2} 
\leq O(2^{-q}q^{-p/2}),
\end{equation*}
where the coefficients inside the $ O $-notation are only a function of $ p $ and not of $ q $. \qedhere
\end{proof}

\begin{corollary}
\label{cor:piecewise_constant_constructions}
Using a rotationally symmetric piecewise constant approximation of $ \Phi^{-1} $ with intervals $ I_k $, then for approximate Gaussian random variables $ \tilde{Z} $ using constants $ \tilde{Z}_k $ constructed as either
\begin{equation*}
\label{eqt:approximate_normal_expected_value_construction}
\tilde{Z}_k^{L^1} \coloneqq \mathbb{E}(Z\mid \Phi(Z) \in I_k), 
\qquad 
\tilde{Z}_k^\mathrm{C} \coloneqq {\Phi^{-1}}\left(\dfrac{\max I_k + \min I_k}{2}\right), 
\qquad \text{or} \qquad 
\tilde{Z}_k^\mathrm{I} \coloneqq 
\begin{cases}
\Phi^{-1}(\min I_k) & \text{if } \min I_k \geq 0.5 \\
\Phi^{-1}(\max I_k) & \text{if } \max I_k < 0.5, 
\end{cases}
\end{equation*}
then all finite moments of $ \lvert\tilde{Z}\rvert $ are uniformly bounded, and conditions~(\ref{con:symmetry}--\ref{con:bounded_asymptotic_growth}) of theorem~\ref{thm:piecewise_constant_approximation_error} are satisfied. 
\end{corollary}

\begin{proof}
We begin by considering the values defined by $ \tilde{Z}_k^{L^1} $ and let $ \tilde{Z} \equiv  \tilde{Z}_k^{L^1}$. As the approximation is rotationally symmetric, we consider the domain $ (\tfrac{1}{2}, 1) $ where $ Z $ and $ \tilde{Z} $ are both positive. Letting the interval $ I_k $ lie in this domain, then from Jensen's inequality and the law of iterated expectations we obtain 
\begin{equation*}
\mathbb{E}(\tilde{Z}^n) = \mathbb{E}((\mathbb{E}_{I_k}(Z \mid \Phi(Z) \in I_k))^n) \leq \mathbb{E}(\mathbb{E}_{I_k}(Z^n \mid \Phi(Z) \in I_k)) =  \mathbb{E}(Z^n) < \infty,
\end{equation*}
for any $ 1\leq n < \infty $, where the last inequality is a standard result \citep[C.2]{blundell2014concepts}. Mirroring this result to the full domain $ (0, 1) $, we can directly see that $ \lvert \tilde{Z} \rvert $ is uniformly bounded. Furthermore, as $ \Phi^{-1} $ is convex in $ [0.5, 1) $, then by the Hermite-Hadamard inequalities the value $ \tilde{Z}_k^{L^1} $ is an upper bound on $ \tilde{Z}_k^\mathrm{C} $ and $ \tilde{Z}_k^\mathrm{I} $, and so these too are uniformly bounded. 

As $ \tilde{Z}_k^{L^1} $ is an upper bound for these choices, it will suffice to show that this satisfies conditions~(\ref{con:symmetry}--\ref{con:bounded_asymptotic_growth}).  Continuing with $ \tilde{Z} \equiv  \tilde{Z}_k^{L^1}$, it is immediately clear that conditions~(\ref{con:symmetry}) and (\ref{con:intermediate_values}) satisfied, so we only need to show condition~(\ref{con:bounded_asymptotic_growth}) is met. The lower bound in condition~(\ref{con:bounded_asymptotic_growth}) is immediately satisfied, so to show the upper bound is too, we consider the difference between the smallest value in the final $ I_K $ interval, namely $ z_K $, and the interpolation value $ Q_K $ where $ Q_K = \mathbb{E}(Z \mid \Phi(z) \in I_K) $. Inspecting the difference we obtain
\begin{equation*}
Q_K - z_K = 2^q \int_{u_K}^{1} (\Phi^{-1}(u) - \Phi^{-1}(u_K) \dd{u} = 2^q \int_{z_K}^{\infty} (z - z_K) \phi(z) \dd{z} \lessapprox  2^q \dfrac{\phi(z_K)}{z_K^2} \lessapprox \dfrac{1}{z_K} (1 - z_K^{-2})^{-1} \leq \dfrac{4.093}{z_K},
\end{equation*}
where the first inequality follows from lemma~\ref{lemma:approximate_moments}, the second from lemma~\ref{lemma:approximate_tail_values}, and the last uses $ (1 - z_K^{-2})^{-1} \leq 4.093 $ for $ q \geq 3 $. We bound the final $ \tfrac{1}{z_K} $ term using lemma~\ref{lemma:approximate_tail_values} to give
\begin{equation*}
\dfrac{1}{z_K} \lessapprox \frac{1}{\sqrt{q\log(4)}} \left(1 + \frac{\log(q\pi\log(16))}{q\log(16)}\right) \leq  \frac{1}{\sqrt{q\log(4)}}\left(1 +  \frac{\mathrm{\pi}}{\mathrm{e}}\right) <  \frac{1.831}{\sqrt{q}}
\end{equation*}
for $ q \geq 0.313 $, where the factor $ \tfrac{\pi}{\mathrm{e}} $ comes from differentiating the first parenthesised expression with respect to $ q $, with its maximum at $ q = \tfrac{\mathrm{e}}{\pi\log(16)} \approx 0.312 $. Using this bound we obtain $ Q_K \leq z_K + 7.5 q^{-1/2} $ for $ q \geq 3 $. \qedhere
\end{proof}

We can remark that the $ \tilde{Z}_k^\mathrm{C} $ construction from corollary~\ref{cor:piecewise_constant_constructions} is equivalent to the central truncated value produced from the framework by \citet[(4)]{giles2019random_quadrature}. In any piecewise constant approximations used, such as that in figure~\ref{fig:piecewise_constant_approximation}, we will use the constants defined by  the $ \tilde{Z}_k^{L^1} $ construction from corollary~\ref{cor:piecewise_constant_constructions}. Furthermore, we can see that our bound from theorem~\ref{thm:piecewise_constant_approximation_error} appears tight in figure~\ref{fig:piecewise_constant_gaussian_approximation_error}.

\subsection{Piecewise linear approximations on geometric intervals}
\label{sec:piecewise_linear_approximations_on_geometric_intervals}

Looking at the piecewise constant approximation in figure~\ref{fig:piecewise_constant_gaussian_approximation}, it is clear there are two immediate improvements that can be made. The first is to use a piecewise linear interpolation, which is considerably more appropriate for the central region. Secondly, the intervals should not be of equal sizes, but denser near the singularities. We will make both these modifications in a single step, where we will construct a piecewise linear approximation with geometrically decaying intervals widths which are dense near the singularities. For brevity we will denote this just as the piecewise linear approximation. An example piecewise linear approximation using 8 intervals is shown in figure~\ref{fig:piecewise_linear_gaussian_approximation}. The precise nature of the interval widths, and how the linear functions are fitted will be detailed shortly, but from a direct comparison against figure~\ref{fig:piecewise_constant_gaussian_approximation} it is clear that the fidelity of a piecewise linear approximation is much better than the piecewise constant. 

The main result from this section will be theorem~\ref{thm:piecewise_linear_approximation_error}, which will bound the $ L^p $ error of our piecewise linear approximation. The proof will proceed in a similar fashion the proof of theorem~\ref{thm:piecewise_constant_approximation_error}, where we will bound sum of the central intervals and the end intervals separately. For the central intervals we will use the Peano kernel theorem to bound the point wise error, and in the end intervals several results will be a mixture of exact results and bounds from lemmas~\ref{lemma:approximate_tail_values} and \ref{lemma:approximate_moments}.

\begin{figure}[htb]
\centering

\hfill
\subfigure[A piecewise linear approximation using 8  intervals.\label{fig:piecewise_linear_gaussian_approximation}]{\includegraphics{piecewise_linear_gaussian_approximation}} \hfill 
\subfigure[The $ L^2 $ error for various polynomial orders, with the number of intervals in $ (0, \tfrac{1}{2}) $ labeled.\label{fig:piecewise_linear_gaussian_approximation_error}]{\includegraphics{piecewise_linear_gaussian_approximation_error}}\hfill

\caption{The piecewise linear approximation and its error from theorem~\ref{thm:piecewise_linear_approximation_error} with geometric intervals using $ r = \tfrac{1}{2} $.}
\label{fig:piecewise_linear_approximation}
\Description[The piecewise linear approximation is of a good fidelity and its error decreases with more intervals and higher polynomial orders.]{The piecewise linear approximation with 8 intervals fits with a high fidelity the exact inverse Gaussian cumulative distribution function. We consider the root mean square error, which decreases as the number of intervals or polynomial order is increased.}
\end{figure}

\begin{theorem}
\label{thm:piecewise_linear_approximation_error}
For a rotationally symmetric approximation $ D \approx \Phi^{-1}$, with $ K $ intervals in $ (0, \tfrac{1}{2}) $, we define the $ k $-th interval $ I_k \coloneqq [\tfrac{r^k}{2}, \tfrac{r^{k-1}}{2})$ for $ k \in \{1,2,\ldots, K-1\} $ and $ I_K \coloneqq (0, \tfrac{r^{K-1}}{2}) $ for some decay rate $ r \in(0,1) $. Each interval uses a piecewise linear approximation $ D_k(u) \equiv D(u) $ for any $ u \in I_k $. The gradient and intercept in each interval is set by the $ L^2 $ minimisation $ D_k \coloneqq \argmin_{D' \in \mathcal{P}_1} \int_{I_k}\lvert\Phi^{-1}(u) - D'(u)\rvert^2 \dd{u} $ where $ \mathcal{P}_1 $  is the set of all 1-st order polynomials. Then we have for any $ 2 \leq p < \infty  $
\begin{equation*}
\lVert D - \Phi^{-1}\rVert_p^p 
= O((1-r)^{2p}) +
O(r^{K-1} {\log}^{-p/2}(r^{1-K}\sqrt{2/\pi}))  = O((1-r)^{2p}) +
o(r^{K-1}).
\end{equation*}
\end{theorem}

\begin{proof}
Considering the domain $ (0, \tfrac{1}{2}) $, we split the contribution into those from the intervals without the singularity, and that from the final interval with the singularity, where
\begin{equation*}
\lVert D - \Phi^{-1}\rVert_p^p   = 2  \sum_{k=1}^{K-1} \int_{I_k} \lvert D_k(u) - \Phi^{-1}(u)\rvert^p \dd{u}  + 2 \int_{I_K} \lvert D_K(u) - \Phi^{-1}(u)\rvert^p \dd{u},
\end{equation*}
where the factors of 2 correct for us splitting the domain $ (0, 1) $ into $ (0, \tfrac{1}{2}) $. 

Beginning with the non singular intervals, we express the point wise error using the Peano kernel theorem \citep{iserles2009first,powell1981approximation}, which we will later bound. For notational simplicity, we denote the approximated functioned as $ f $, where $ f \equiv \Phi^{-1} $, and a given interval as $ [a,b] \equiv I_k  $. The $ L^2 $ optimal linear approximation is $ \alpha(f) + \beta(f) u $ for $ u \in [a,b] $ where $ \alpha $ and $ \beta $ are functionals. The point wise error $ f(u) - \alpha(f) - \beta(f)u $ is a linear mapping $ L $ acting on $ f $ where $ L(f)(u) \coloneqq  f(u) - \alpha(f) - \beta(f)u $. By construction $ L $ annihilates linear functions, so the Peano kernel is $ k(\xi; u) \coloneqq (u - \xi)^+ - \alpha((\cdot - \xi)^+) - \beta((\cdot - \xi)^+)u \equiv (u - \xi)^+ - \bar{\alpha}(\xi) - \bar{\beta}(\xi)u $ for $ \xi \in [a,b] $ where defined $ \bar{\alpha}(\xi) \coloneqq \alpha((\cdot - \xi)^+) $ and similarly $ \bar{\beta}(\xi) $. The point wise error is $ \varepsilon(u) \coloneqq L(f)(u) = \int_{a}^{b} k(\xi; u) f''(\xi) \dd{\xi} $.

To determine the kernel's intercept and gradient, we use that they are $ L^2 $ optimal, and so the functional derivatives of $ \int_a^b \varepsilon^2(u) \dd{u} $ with respect to $ \alpha $ and $ \beta $ are zero, giving the simultaneous equations
\begin{equation*}
\alpha(f)(b-a) + \beta(f) \left(\dfrac{b^2 - a^2}{2}\right)  = \int_{a}^{b} f(u) \dd{u} 
\qquad \text{and} \qquad 
\alpha(f)\left(\dfrac{b^2 - a^2}{2}\right) + \beta(f) \left(\dfrac{b^3 - a^3}{3}\right)  = \int_{a}^{b} u f(u) \dd{u}.
\end{equation*}
It is important to notice that because we chose the $ L^2 $ norm, these are a set of linear simultaneous equations, and thus $ \alpha $ and $ \beta $ are linear functionals, thus showing that $ L $ is linear, (a requirement of the Peano kernel theorem). Evaluating these for the kernel function ($ f \to (\cdot - \xi)^+  $) gives
\begin{equation*}
\bar{\alpha}(\xi)  = - \dfrac{(b - \xi)^2 ((b+a)\xi - 2a^2)}{(b - a)^3} 
\qquad \text{and} \qquad 
\label{eqt:peano_kernel_coefficient}
\bar{\beta}(\xi) = \dfrac{(b - \xi)^2 (2\xi + b - 3a)}{(b - a)^3}.
\end{equation*}
Thus, the point wise error is 
\begin{equation*}
\varepsilon(u)  = \int_{a}^{b} ((u - \xi)^+ - \bar{\alpha}(\xi) - \bar{\beta}(\xi) u ) f''(\xi) \dd{\xi} = (b - a)^2 \int_{0}^{1} ((\tilde{u} - \tilde{\xi})^+ - (1 - \tilde{\xi}^2)(\tilde{\xi} + \tilde{u})) f''((b - a)\tilde{\xi} + a) \dd{\tilde{\xi}},
\end{equation*}
where to achieve the last equality we rescaled our interval $ [a, b] \to [0, 1] $  and variables $ \eta \to \tilde{\eta}  $ where $ \tilde{\eta} \coloneqq \tfrac{\eta - a}{b - a} $. Taking the absolute value and applying Jensen's inequality immediately gives 
\begin{equation*}
\lvert \varepsilon(u) \rvert \leq - 6 (b - a)^2  {\dv[2]{}{u}}\Phi^{-1}(a) = - 6 a^2 \left(\dfrac{1-r}{r}\right)^2  {\dv[2]{}{u}}\Phi^{-1}(a) \leq 2.59 \left(\dfrac{1-r}{r}\right)^2,
\end{equation*}
where for the first inequality we used that $ {\dv[2]{}{u}}\Phi^{-1} $ is maximal at the lower boundary, and for the second inequality we bound this by the maximum with respect to $ a $ (at 0.177). Using this expression for the point wise error in our summation of the non-singular intervals gives (as $ r \to 1 $)
\begin{equation*}
 \sum_{k=1}^{K-1} \int_{I_k} \lvert D_k(u) - \Phi^{-1}(u)\rvert^p \dd{u}
= \sum_{k = 1}^{K - 1} \int_{I_k} \lvert\varepsilon(u)\rvert^p \dd{u}
\leq 2.59^p \left(\dfrac{1 - r}{r}\right)^{2p}  \int_{0}^{\frac{1}{2}} \dd{u}
= O((1-r)^{2p}).
\end{equation*}

We now consider the interval $ [0, b] \equiv I_K $ containing the singularity at 0, which has intercept and gradient
$ \alpha = \tfrac{1}{b} \int_{0}^{b} \Phi^{-1}(u) \dd{u} - \tfrac{b\beta}{2}  $
and 
$ \beta  = \tfrac{12}{b^3} \int_{0}^{b} (u - \tfrac{b}{2}) \Phi^{-1}(u) \dd{u} $.
These integrals can be calculated exactly (with a change of variables), where denoting $ z_b \coloneqq \Phi^{-1}(b) $ we obtain
\begin{equation*}
\alpha = \dfrac{2\phi(z_b)}{b} - \dfrac{3\Phi(\sqrt{2}z_b)}{b^2\sqrt{\pi}}
\qquad \text{and} \qquad 
\beta  = \dfrac{6}{b^3} \left(\dfrac{\Phi(\sqrt{2} z_b)}{\sqrt{\pi}} - b\phi(z_b)\right).
\end{equation*}
For the gradient, as the intervals becomes ever smaller and $ b \to 0 $, we can use lemma~\ref{lemma:approximate_tail_values} to give
\begin{equation*}
\beta  
\approx -\dfrac{6}{b^3} \left(\phi^2(z_b)\left(\dfrac{1}{z_b} - \dfrac{1}{2 z_b^3}\right) + b \phi(z_b)\right)
\approx -\dfrac{6z_b}{b}\left(\dfrac{1}{2z_b^2} + O(z_b^{-4})\right)
\approx \dfrac{-3}{bz_b},
\end{equation*}
where in the first approximation we used  $ \phi(\sqrt{2}z) \equiv \sqrt{2\pi}\phi^2(z) $. Interestingly, this means the range of values $ \beta b \approx -\tfrac{3}{z_b} \to 0 $, and our approximation ``flattens'' relative to the interval $ [0, b] $ as $ b \to 0 $.

With the intercept and gradient in the singular interval $ I_K $ known exactly, we define the two points $ u_- $ and $ u_+ $ where the error is zero, where $ 0 < u_- < u_+ < b $, and there are two as $ \Phi^{-1} $ is concave in $ (0, \tfrac{1}{2}) $. Corresponding to these we define $ z_- \coloneqq \Phi^{-1}(u_-) $ and $ z_+ \coloneqq \Phi^{-1}(u_+) $, where $ -\infty < z_- < z_+ < z_b < 0 $. Thus in the singular interval we have
\begin{equation*}
\int_{I_K} \lvert D_K(u) - \Phi^{-1}(u)\rvert^p \dd{u} 
= \int_{-\infty}^{z_-} \lvert z - \alpha - \beta \Phi(z)\rvert^p \phi(z) \dd{z} + \int_{z_-}^{z_b} \lvert z - \alpha - \beta \Phi(z)\rvert^p \phi(z) \dd{z}.
\end{equation*}
Using lemmas~\ref{lemma:approximate_tail_values} and \ref{lemma:approximate_moments}, then for the first of these integrals we obtain
\begin{equation*}
\int_{-\infty}^{z_-} \lvert z - \alpha - \beta \Phi(z)\rvert^p \phi(z) \dd{z}
\leq \int_{-\infty}^{z_-} \lvert z - z_-\rvert^p \phi(z) \dd{z}
\leq \int_{-\infty}^{z_b} \lvert z - z_b\rvert^p \phi(z) \dd{z}
\approx  \dfrac{p!\phi(z_b)}{\lvert z_b\rvert^{p+1}}
=  O\left(\dfrac{b}{\lvert z_b \rvert^p}\right),
\end{equation*}
and for the second integral we similarly obtain
\begin{equation*}
\int_{z_-}^{z_b} \lvert z - \alpha - \beta \Phi(z)\rvert^p \phi(z) \dd{z}
\leq \int_{-\infty}^{z_b} \lvert z_+ - \alpha - \beta \Phi(z)\rvert^p \phi(z) \dd{z}
\leq \lvert \beta b \rvert^p \int_{0}^{b}\dd{u}
\approx \dfrac{3^p}{\lvert z_b\rvert^p} b
=  O\left(\dfrac{b}{\lvert z_b \rvert^p}\right).
\end{equation*}

Combining the results for the central intervals and the singular interval we obtain
\begin{equation*}
\lVert D - \Phi^{-1}\rVert_p^p
= O((1-r)^{2p}) + O\left(\dfrac{b}{\lvert z_b \rvert^p}\right) 
= O((1-r)^{2p}) +
O(r^{K-1} {\log}^{-p/2}(r^{1-K}\sqrt{2/\pi}))  
= O((1-r)^{2p}) +
o(r^{K-1}),
\end{equation*}
where for the second equality we used lemma~\ref{lemma:approximate_tail_values} in the limit $ r^{K-1} \to 0 $. \qedhere
\end{proof}

We can see from theorem~\ref{thm:piecewise_linear_approximation_error} that the $ O((1 - r)^{2p}) $ comes from the central regions, and is reduced by taking $ r \to 1 $, and the $ o(r^{K-1}) $ term is from the singular intervals, and is reduced by taking $ r^{K-1} \to 0 $. The key point of interest with this result is that the error from the central regions and the singular regions are decoupled. In order to decrease the overall error, it is not sufficient to only increase the number of intervals ($ K \to \infty $), which would only improve the error from the singular intervals, but the decay rate must also be decreased ($ r \to 1 $). The independence and interplay of these two errors is important for balancing the fidelity between the central and edge regions.

There are some natural follow on questions concerning the piecewise linear construction presented in theorem~\ref{thm:piecewise_linear_approximation_error}: can we make it continuous, can we use higher order polynomials, and how can we determine the polynomials' optimal coefficients? Consider using higher order polynomials, such as e.g.\ cubics, (recall that our primary aim is speed and simplicity rather than machine precision fidelity), where for an arbitrary interval $ [a,b] $ we trying to approximate $ f $ by a $ m $-th order polynomial $ \tilde{f} $ where $ f(u) \approx \tilde{f}(u) = c_0 + c_1 u + c_2 u^2 + \cdots + c_m u^m $ with $ c_m \neq 0 $ for any $ u \in [a,b] $. Sticking with the $ L^2 $ optimal polynomial, we can minimise the $ L^2 $ error by equating the functional derivates to zero, thus obtaining the set of $ m + 1 $ linear simultaneous equations of the form $ A\bm{x} = \bm{b} $ where: $ A_{i,j} = \tfrac{b^{i+j+1} - a^{i+j+1}}{i+j+1} $, $ \bm{x}_i = c_i $, and $ \bm{b}_{i} = \int_{a}^{b} u^i f(u) \dd{u} $, with indices $ i,j \in \{0,1,2,\ldots,m\} $. Solving this we can fit higher order polynomials, which is how we generated figure~\ref{fig:piecewise_linear_gaussian_approximation_error}. Furthermore, we can trivially use our bound from theorem~\ref{thm:piecewise_linear_approximation_error} to bound higher order polynomials, and thus piecewise quadratic or cubic approximations are permissible in our framework. This then just leaves the question about forming a continuous piecewise linear approximation. Requiring that the approximation is continuous over the entire interval $ (0, 1) $ would turn our previous minimisation into a constrained minimisation problem, with an coupled set of constraints, where we can no longer trivially set the functional derivatives to zero.  While such a continuous approximation may be more aesthetically pleasing, it is of no practical consequence for the inverse transform method, and by definition will have a worse overall $ L^2 $ error than the discontinuous approximation from theorem~\ref{thm:piecewise_linear_approximation_error}, and thus we avoid and discourage imposing such a constraint. 

The errors from piecewise linear approximations for various polynomial orders and interval numbers are shown in figure~\ref{fig:piecewise_linear_gaussian_approximation_error}, where we have set the decay rate $ r = \tfrac{1}{2} $. As we increase the number of intervals used, then for the piecewise linear function, the $ L^2 $ error plateaus to approximately $ 10^{-2} $ for 16 intervals, which is approximately equal to the error for the piecewise constant approximation using $ 10^3 $ intervals. Thus we can appreciate the vast increase in fidelity that the piecewise linear approximation naturally boasts over the piecewise constant approximation. Furthermore, this plateau demonstrates that the central regions are limiting the accuracy of the approximation. Inspecting the approximation using 16 intervals in $ (0, \tfrac{1}{2}) $, we see that as we increase the polynomial order from linear to cubic there is a considerable drop in the error approximately equal to a factor of $ 10^2 $, where the piecewise cubic approximation is achieving $ L^2 $ errors just shy of $ 10^{-4} $. However, we see that increasing the polynomial order any further does not lead to any significant reduction in the $ L^2 $ error, indicating that in this regime it is the error in the singular interval which is dominating the overall error. 

\section{High performance implementations}
\label{sec:high_performance_impementations}

Our motivation for presenting the piecewise constant and linear approximations was to speed up the inverse transform method, where the approximations were inherently simple in their construction, with the suggestion that any implementation would likely be simple and performant. However, it is not obvious that the implementations from various software libraries, especially heavily optimised commercial libraries such and Intel's maths kernel library (MKL), should be slow. In this section, we will mention how most libraries implement these routines, and why many are inherently ill posed for modern vector hardware. Our two approximations capitalise on the features where most libraries stumble, avoiding division and conditional branching, which will be the key to their success. We give a brief overview of their implementations in C and showcase their superior speed across Intel and Arm hardwares. While we will briefly outline the implementations here in moderate detail, a more comprehensive suite of implementations and experiments, including Intel AVX-512 and Arm SVE specialisations, are hosted in a centralised repository by \citet{sheridan2020approximate_random}.

\subsection{The shortcomings in standard library implementations}

Several libraries providers, both freely open-source and commercial, offer implementations of the inverse Gaussian cumulative distribution function, where providers include Intel, NAG, Nvidia, MathWorks, Cephes, GNU, etc. The focus of these implementations is primarily ensuring near machine precision is achieved for all possible valid input values, and handling errors and edge cases appropriately. While some offer single precision implementations, most offer (or default to) double precision implementations, which are typically accurate to 16 significant figures. In order to meet these precisions, algorithms have become ever more improved over the years \citep{hastings1955approximations,evans1974algorithm70,beasley1985percentage,wichura1988algorithm,marsaglia1994rapid,giles2011approximating}, where the most widely used is by \citet{wichura1988algorithm}, which is used in: GNU's scientific library (GSL), the Cephes library (used by Python's SciPy package), and the NAG library (NAG uses a slight modification). 

The \textit{de facto} for these implementations is to split the function into two regions: an easy central region, and a difficult tail region. Inside the central region a rational Pad\'{e} approximation is used, where one seventh order polynomial is divided by another. In the tail regions, the square root of the logarithm is computed, and then a similar rational approximation performed on the transformed variable. While these are very precise routines, they have several shortcomings with respect to performance.

The first shortcoming is the division operation involved in computing the rational approximation. Division is considerably more expensive than addition or multiplication, and with a much higher latency \citep{wittmann2015short,fog2018instruction}. Thus, for the very highest performance, avoiding division is preferable, and thus our polynomial approximations. (The implementation by \citet{giles2011approximating} already capitalises on this). Similarly, taking the logarithm and square root are expensive, making the tail regions very expensive, and so are best avoided too. 

The second short coming is that the routine branches, requesting a very expensive calculation for the occasional tail values, and a less expensive calculation for more frequent central values. Branching is achieved by using ``\texttt{if-else}'' conditions, but unfortunately, this often inhibits vectorisation, resulting in compilers being unable to issue single instruction multiple data (SIMD) operations \citep{vanderpas2017using}. Even if a vectorised SIMD implementation is produced, typically the results from both branches are computed, and the correct result selected by predication/masking. Thus, irrespective of the input, both the very expensive and less expensive calculation may be performed, resulting in the relatively infrequent tail values producing a disproportionately expensive overall calculation. This effect is common known as \emph{warp divergence} in GPU programs, and in our setting we will call this factor \emph{vector divergence}. It is important to highlight that the wider the vector and the smaller the data type, and thus the greater the amount of parallelisation by vectorisation, the worse the impact of vector divergence. Noting that half precision uses only 16 bits, both Intel's AVX-512 and Fujitsu's A64FX are 512 bits wide, and Arm's SVE can be up to 2048 bits \citep{petrogalli2016sneak_peak,stephens2017arm}, the degree of vector parallelisation can be vast, and the impact from conditional branching becomes evermore crippling as vector parallelisation increases. 

In light of these two shortcomings, we will see our approximations can be implemented in a vectorisation friendly manner which avoids conditional branching, and are homogenous in their calculations. Furthermore, their constant and polynomial designs avoid division operations, and only require addition, multiplication, and integer bit manipulations, and thus result in extremely fast executables. We will see that our piecewise constant approximation will rely on the high speed of querying the cache. Furthermore, for the piecewise linear approximation using a decay rate $ r = \tfrac{1}{2} $ and 15 intervals, then in single precision all the coefficients can be held in 512 bit wide vector registers, bypassing the need to even query the cache, and thus are extremely fast provided the vector widths are sufficiently large. 

\subsection{Implementing the piecewise constant approximation}

The piecewise constant is easy to implement. Splitting the domain $ (0, 1) $ into the $ N $ intervals $ [\tfrac{m}{N}, \tfrac{m+1}{N}) $ zero indexed by $ m \in \{0,1,2,\ldots,N-1\}$, the values for each interval are easily computed \textit{a priori}, and stored in a lookup table. An example of how this looks in C is shown in code~\ref{code:piecewise_constant_approximation}, where we use OpenMP (\texttt{omp.h}) to signal to the compiler that the \texttt{for} loop is suitable for vectorisation. This relies on the typecasting to the integer, where any fractional bit is removed. The benefit to such an implementation is that on modern chips a copy of the lookup table can readily fit within either the L1 or L2 caches, where 1024 values stored in 64 bit double precision consume \SI{8}{\kilo\byte}. As an example, an Intel Xeon Gold 6140 CPU has L1 and L2 caches which ware \SI{32}{\kilo\byte} and \SI{2014}{\kilo\byte} respectively, and thus the lookup table can exploit the fast speeds of the L1 cache, which typically has latencies of 2--5 clock cycles. 

\begin{lstfloat}[htb]
\begin{lstlisting}[style=C, caption={C implementation of the piecewise constant approximation.}, label={code:piecewise_constant_approximation}]
#define LOOKUP_TABLE_SIZE 1024
const double lookup_table[LOOKUP_TABLE_SIZE] = {-3.3, -2.9, -2.8, ..., 2.8, 2.9, 3.3};

void piecewise_constant_approximation(const unsigned int n_samples, 
                                      const double * restrict input, 
                                      double * restrict output) {
    #pragma omp simd
    for (unsigned int n = 0; n < n_samples; n++) 
        output[n] = lookup_table[(unsigned int) (LOOKUP_TABLE_SIZE * input[n])];
}
\end{lstlisting}
\end{lstfloat}

\subsection{Implementing the piecewise linear approximation}

The piecewise linear approximation is a bit more involved that the piecewise constant approximation. Not only do we have a polynomial to evaluate (albeit only linear), but the geometric and varying widths of the intervals, identifying which interval a given value corresponds to appears more involved. Once the appropriate interval's polynomials coefficients are found, evaluating the polynomial is trivially, where the linear polynomial can be evaluated using a single fused multiply and add (FMA) instruction. Higher order polynomials can be similarly computed (e.g.\ with Horner's rule). 

The primary challenge is determining which interval a given value corresponds to based off of the construction from theorem~\ref{thm:piecewise_linear_approximation_error} for a given decay rate $ r $. As floating point numbers are stored in binary using their sign, exponent, and mantissa, the natural choice most amenable for computation is $ r = \tfrac{1}{2} $, which we call the \emph{dyadic} rate, producing the dyadic intervals shown in table~\ref{tab:dyadic_intervals}. Looking at table~\ref{tab:dyadic_intervals}, we notice that the intervals are only dense near the singularity at 0, but not near the singularity at 1. This is not problematic, as we said in theorem~\ref{thm:piecewise_linear_approximation_error} that the approximation is rotationally symmetric, and thus we can use the $ N $ intervals in $ (0, \tfrac{1}{2}) $, and if our input value is within $ (\tfrac{1}{2}, 1) $, it is straight forward to simply negate the value computed for the input reflected about $ \tfrac{1}{2} $ (equivalent to using the inverse complementary cumulative distribution function). 

\begin{table}[htb]
\centering
\caption{Dyadic intervals and their corresponding array indices.}
\label{tab:dyadic_intervals}
\renewcommand{\arraystretch}{1.4}  % This is only local to this one table.
\begin{tabular}{c|ccccccc}
Index & 0 & 1 & 2 & $ \cdots $ & $ n $ & $ \cdots $ & $ N $ \\ \hline
Interval & $ [\tfrac{1}{2}, 1) $ & $ [\tfrac{1}{4}, \tfrac{1}{2}) $ & $ [\tfrac{1}{8}, \tfrac{1}{4}) $ & $ \cdots $ & $ [\tfrac{1}{2^{n+1}}, \tfrac{1}{2^n}) $ & $ \cdots $ & $ (0, \tfrac{1}{2^N}) $
\end{tabular}
\end{table}

Such an implementation can handle any value in $ (0, \tfrac{1}{2}) \cup (\tfrac{1}{2}, 1) $, but unfortunately $ \tfrac{1}{2} $ is not included within this range. While a mathematician may say the individual value $ \tfrac{1}{2} $ has zero measure, from a computational perspective such a value is perfectly feasible input, and quite likely to appear in any software tests. Thus our implementation will correctly handle this value. The reader will notice that when we later reflect an input value $ x $ about $ \tfrac{1}{2} $ by using $ x \to 1 - x $, then the value $ \tfrac{1}{2} $ will not be in any of the intervals indexed between 1--$ N $ in table~\ref{tab:dyadic_intervals}, but remains in the interval indexed by 0. Thus in our later implementations, the arrays of coefficients will always hold as their first entry (index 0) zero values so $ \tfrac{1}{2} $ is correctly handled. We found during the development of the implementation, being able to correctly handle this value is of considerable practical importance, so so encourage practitioners to also correctly handle this value. 

If we are using the dyadic decay rate, then the interval an input $ x $  belongs to is $ \lceil -{\log}_2(x) \rceil - 1 $. However, to compute this we needn't take any logarithms, which are expensive to compute. We can obtain the same result by reading off the exponent bits in the floating point representation, treating these as an integer, and then correcting for the exponent bias.  This only involves simple bit manipulations, and interpreting the bits in a floating point representation as an integer. In C this can either by achieved by a technique called type-punning, either by pointer-aliasing, using a \texttt{union}, or using specialised intrinsic functions. Of these, pointer aliasing technically breaks the strict aliasing rule in C (and C++) \citep[6.5.2.3]{iso2012c11} \citep[pages~163--164]{stallman2020gcc}. Similarly, type-punning with a \texttt{union} in C89 is implementation defined, whereas in C11 the bits are re-interpreted as desired. In our implementation we will leave this detail undefined and just use a macro to indicate this aliasing. 

\begin{algorithm}[h!tb]
\DontPrintSemicolon
\KwIn{Floating-point uniform random variable $ U \in [0, 1) $.}
\KwOut{Floating-point approximate Gaussian random variable $ \tilde{Z} $.}
Form predicate using $ U > \tfrac{1}{2} $.\;
Reflect about $ \tfrac{1}{2} $ to obtain $ U \in [0, \tfrac{1}{2}] $.\;
Alias $ U $ as an unsigned integer.\;
Read the exponent bits using bitwise-AND.\;
Right shift away the mantissa's bits.\;
Obtain an array index by correcting for exponent bias.\;
Cap the array index to avoid overflow.\;
Read the polynomial coefficients.\;
Re-interpret $ U $ as a float.\;
Form the polynomial approximation $ \tilde{Z} $.\;
Correct sign of approximation based on the predicate.\;
\caption{Piecewise polynomial approximation using dyadic intervals.}
\label{algo:piecewise_polynomial_approximation_using_dyadic_intervals}
\end{algorithm}

Overall then, the general construction from theorem~\ref{thm:piecewise_linear_approximation_error} is given in algorithm~\ref{algo:piecewise_polynomial_approximation_using_dyadic_intervals}. Furthermore, a single precision C implementation is shown in code~\ref{code:piecewise_linear_approximation}, where we assume the single precision floats are stored in IEEE 32 bit floating point format \citep{ieee2008ieee}. 

\begin{lstfloat}[h!tb]
\begin{lstlisting}[style=C, caption={C implementation of the piecewise linear approximation.}, label={code:piecewise_linear_approximation}]
typedef unsigned int uint32;  // Assuming 32 bit floats. 
typedef float float32;        // Assuming 32 bit integers. 

#define TABLE_SIZE 16
#define N_MANTISSA_32 23            // For IEEE 754
#define FLOAT32_EXPONENT_BIAS 127   // For IEEE 754
#define FLOAT32_EXPONENT_BIAS_TABLE_OFFSET (FLOAT32_EXPONENT_BIAS - 1)
#define TABLE_MAX_INDEX (TABLE_SIZE - 1) // Zero indexing.
#define FLOAT32_AS_UINT32(x) (...)       // Type-punning.

const float32 poly_coef_0[TABLE_SIZE] = {0.0, -1.3, -1.6, ..., -4.0, -4.1, -4.5};
const float32 poly_coef_1[TABLE_SIZE] = {0.0, 2.6, 3.7, ..., 2800.0, 5300.0, 21000.0};

#pragma omp declare simd
static inline float32 polynomial_approximation(const float32 u, const uint32 index) {
    return poly_coef_0[index] + poly_coef_1[index] * u;
}

#pragma omp declare simd
static inline uint32 get_table_index_from_float_format(const float32 u) {
    uint32 index = FLOAT32_AS_UINT32(u) >> N_MANTISSA_32;      // Remove the mantissa.
    index = FLOAT32_EXPONENT_BIAS_TABLE_OFFSET - index;        // Get the index.
    return index > TABLE_MAX_INDEX ? TABLE_MAX_INDEX : index;  // Avoid overflow.
}

void piecewise_polynomial_approximation(const unsigned int n_samples,
                                        const float32 * restrict input, 
                                        float32 * restrict output) {
    #pragma omp simd 
    for (unsigned int i = 0; i < n_samples; i++) {
        float32 u = input[i];
        bool predicate = u < 0.5;
        u = predicate ? u : 1.0 - u;
        uint32 index = get_table_index_from_float_format(u);
        float32 z = polynomial_approximation(u, index);
        z = predicate ? z : -z;
        output[i] = z;
    }
}
\end{lstlisting}
\end{lstfloat}

The reason why we decide to implement the approximation in single precision using coefficient arrays of 16 entries is because this requires 512 bits ($ 16 \times 32 $ bits) to store all the possible values for a given monomial's coefficient. The significance of 512 bits can be understated, as it is the width of an AVX-512 and A64FX vector register. Thus, instead of querying the cache to retrieve the coefficients, they can be held in vector registers, bypassing the cache entirely, and achieving extremely fast speeds. Recognising the coefficients can be stored in a single coalesced vector register is currently largely beyond most compilers' capabilities using only OpenMP directives and compiler flags alone. However, in the repository by \citet{sheridan2020approximate_random}, specialised implementations using Intel vector intrinsics and Arm inline assembly achieve this, obtain the ultimate in performance. 

\subsection{Performance of the implementations}

Both the piecewise constant and linear approximation implementations that we  presented in codes~\ref{code:piecewise_constant_approximation} and \ref{code:piecewise_linear_approximation} are non branching, vector capable, and use only basic arithmetic and bit wise operations, and thus we anticipate their performance should be exceptionally good. Indeed, their performance, along with several other implementations are shown in table~\ref{tab:implementation_times}, with experiments performed on an Intel Xeon Gold (Skylake) CPU and an Arm based Cavium ThunderX2 \citep{sheridan2020approximate_random}.

\begin{table}[htb]
\caption{Performance of various $ \Phi^{-1} $ implementations and approximations.}
\label{tab:implementation_times}
\begin{tabular}{lccccc}
Description & Implementation & Hardware & Compiler & Precision & Time (clock cycles)\\ 
\hline
Cephes  \citep{moshier1992cephes} & --- &  Intel & \texttt{icc} & Double & $ 60 \pm 1 $ \\
GNU GSL & --- &  Intel & \texttt{icc} & Double & $ 52 \pm 10 $ \\
ASA241  \citep{wichura1988algorithm,burkardt2020software} & --- &  Intel & \texttt{icc} & Single & $ 47 \pm 1 $ \\
\citet{giles2011approximating} & --- & Intel & \texttt{icc} & Single & $ 46 \pm 2 $ \\
Intel (HA) & Intel MKL VSL &  Intel & \texttt{icc} & Double &  $ 9 \pm 0.5 $ \\
Intel (LA)   &Intel MKL VSL &  Intel & \texttt{icc} & Double &   $ 7 \pm 0.5 $ \\
Intel (HA) & Intel MKL VSL &  Intel & \texttt{icc} & Single &  $ 3.4 \pm 0.1 $ \\
Intel (LA)   &Intel MKL VSL &  Intel & \texttt{icc} & Single &   $ 2.6 \pm 0.1 $ \\
Piecewise constant  & OpenMP & Arm & \texttt{armclang} & Double & $ 4.0 \pm 0.5 $ \\
Piecewise constant  & OpenMP & Intel & \texttt{icc} & Double & $ 1.5 \pm 0.3 $ \\
Piecewise cubic  & OpenMP &   Intel & \texttt{icc} & Single &   $ 0.9 \pm 0.1 $  \\
Piecewise cubic    & Intel intrinsics  &   Intel & \texttt{icc} & Single &  $ 0.7 \pm 0.1 $ \\
Piecewise linear& Intel intrinsics  &   Intel & \texttt{icc} & Single &   $ 0.5  \pm 0.1 $ \\
Read and write & --- & Intel & $ \texttt{icc} $ & Single & $ 0.4 \pm 0.1 $ 
\end{tabular}
\end{table}

Looking at the results from table~\ref{tab:implementation_times}, of all of these, we can see that the piecewise linear approximation implemented using Intel vector intrinsics achieves the very fastest speeds, closely approaching the maximum speed of a simple read and write procedure. Unsurprisingly, the freely available implementations from Cephes and GSL are not competitive with the commercial offerings from Intel. Nonetheless, even in single precision, our approximations, on Intel hardware, consistently beat the performance achieved from the Intel high accuracy (HA) or low accuracy (LA) offerings. Comparing the high accuracy Intel offering and the piecewise linear implementation, there stands to be a speed up by a factor of seven by switching to our approximation. These results vindicate our efforts, and that our simple approximations offer considerable speed improvements. It is also needless to say, that compared to the freely available open-source offerings, the savings become vast. 

\section{Multilevel Monte Carlo}
\label{sec:multilevel_monte_carlo}

One of the core uses of our high speed approximate random variables is in Monte Carlo applications. Frequently, the Monte Carlo method is used to estimate expectations of the form $ \mathbb{E}(P) $ of functionals $ P $ which act on solutions $ X $ of stochastic differential equations of the form $ \dd{X_t} = a(t, X_t) \dd{t} + b(t, X_t)\dd{W_t} $ for given drift and diffusion processes $ a $ and $ b $. The underlying stochastic process $ X_t $ is itself approximated by some $ \hat{X} $ using an appropriate numerical method such as the Euler-Maruyama or Milstein schemes \citep{asmussen2007stochastic,kloeden1999numerical,lord2014introduction}. Approximations such as the Euler-Maruyama and Milstein schemes simulate the stochastic process from time $ t = 0 $ to $ t = T $ over $ N $ time steps of size $ \Delta t = \delta = \tfrac{T}{N} $, where the incremental update at the $ n $-th iteration requires a Gaussian random variable $ Z_n $ to simulate the underlying Wiener process $ W_t $, where $ \Delta W_n = \sqrt{\delta}Z_n $. Such types of Monte Carlo simulations are wide spread, with the most famous situation being to price financial options. 

Approximate random variables come into this picture by substituting the exact random variable samples in the approximation scheme with approximate ones. This facilitates running faster simulations, at the detriment of introducing some error. However, using the multilevel Monte Carlo method \citep{giles2008multilevel}, this error can be compensated for with negligible cost. Thus, the speed improvements offered by switching from exact to approximate random variables can be largely recovered, the original accuracy can be maintained. A detailed inspection of the error introduced from incorporating approximate Gaussian random variables into the Euler-Maruyama scheme and the associated multilevel Monte Carlo analysis is presented by \citet{giles2020approximate}. As such, we will only briefly review the key points of the setup, and detail the resultant computational savings that can be expected from using approximate random variables in a multilevel Monte Carlo setting. 

For the Euler-Maruyama scheme, the unmodified version using exact Gaussian random variables $ Z $ produced an approximation $ \hat{X} $, whereas the modified scheme using approximate random variables $ \tilde{Z} $ produces an approximation $ \tilde{X} $, where the two schemes are respectively
\begin{equation*}
\hat{X}_{n+1} = \hat{X}_n + a(t_n, \hat{X}_n) \delta + b(t_n, \hat{X}_n)\sqrt{\delta} Z_n
\qquad \text{and} \qquad 
\tilde{X}_{n+1} = \tilde{X}_n + a(t_n, \tilde{X}_n) \delta + b(t_n, \tilde{X}_n)\sqrt{\delta} \tilde{Z}_n,
\end{equation*}
where $ t_n \coloneqq n \delta $. The regular multilevel Monte Carlo construction would vary the discretisation between two levels, producing \emph{fine} and \emph{coarse} simulations. The functional $ P $ would act on each path simulation, producing the fine and coarse approximation of the function $ P^{\mathrm{f}} $ and $ P^{\mathrm{c}} $ respectively. If the path simulation uses the exact random variables we denote these as $ \hat{P}^{\mathrm{f}} $ and $ \hat{P}^{\mathrm{c}} $, and alternatively if it uses approximate random variables as $ \tilde{P}^{\mathrm{f}} $ and $ \tilde{P}^{\mathrm{c}} $. In general there may be multiple fine and coarse levels, so we use $ l $ to index these, where increasing values of $ l $ correspond to fine path simulations. Thus we also introduce the notation for a given $ l $ that $ \hat{P}_l \equiv \hat{P}^{\mathrm{f}} $ and $ \hat{P}_{l-1} \equiv \hat{P}^{\mathrm{c}} $, and similarly $ \tilde{P}_l \equiv \tilde{P}^{\mathrm{f}} $ and $ \tilde{P}_{l-1} \equiv \tilde{P}^{\mathrm{c}} $. If we have levels $ l \in \{0, 1, 2, \ldots, L\} $ and use the convention $ \hat{P}_{-1} \coloneqq \tilde{P}_{-1} \coloneqq 0 $, then \citet{giles2020approximate} suggest the nested multilevel Monte Carlo
\begin{equation*}
E(P) 
\approx
E(\hat{P}_L) 
= 
\sum_{l = 0}^{L} E(\hat{P}_l - \hat{P}_{l-1}) 
= 
\sum_{l = 0}^{L} E(\tilde{P}_l - \tilde{P}_{l-1}) +  E(\hat{P}_l - \hat{P}_{l-1} - \tilde{P}_l + \tilde{P}_{l-1}),
\end{equation*}
where the first approximation is regular Monte Carlo procedure \citep{glasserman2013monte}, the first equality is the usual multilevel Monte Carlo decomposition \citep{giles2008multilevel}, and the final equality is the nested multilevel Monte Carlo framework \citep{giles2020approximate,sheridan2020nested}.
\citeauthor{giles2020approximate} \citep{giles2020approximate,sheridan2020nested} show that the two way differences in the regular and nested multilevel Monte Carlo settings behave identically, and the main result of their analysis is determining the behaviour of the final four way difference's variance \citep[lemmas~4.10 and 4.11]{giles2020approximate} \citep[corollaries~6.2.6.2 and 6.2.6.3]{sheridan2020nested}. They find that for Lipschitz continuous and differentiable functionals that 
\begin{equation*}
\lVert \hat{P}^{\mathrm{f}} - \hat{P}^{\mathrm{c}} - \tilde{P}^{\mathrm{f}} + \tilde{P}^{\mathrm{c}}\rVert_p 
\leq O(\delta^{1/2} \lVert Z - \tilde{Z} \rVert_{p'}) 
\end{equation*}
for some  $ p' $ such that $ 2 \leq p < p' < \infty $, and that for Lipschitz continuous but non differential functionals that 
\begin{equation*}
\lVert \hat{P}^{\mathrm{f}} - \hat{P}^{\mathrm{c}} - \tilde{P}^{\mathrm{f}} + \tilde{P}^{\mathrm{c}}\rVert_p 
\leq O(\min\{
\delta^{1/2} \lVert Z - \tilde{Z} \rVert_{p'}^{p'(1-\epsilon)/(p'+1)},
\delta^{(1-\epsilon)/2p -1/2p'}    \lVert Z - \tilde{Z} \rVert_{p'}
\})
\end{equation*}
for some  $ \epsilon $ such that $ 0<\epsilon<1 - \tfrac{p}{p'} $. We can see that in all circumstances covered by their analysis that there is a dependence on the approximation error $  \lVert Z - \tilde{Z} \rVert_{p'} $ for some $ L^{p'} $ norm where $ p' > 2 $. As the lower bound on $ p' $ is strict, we see that the $ L^2 $ norms produced by \citet{giles2019random_quadrature} are insufficient for the nested multilevel Monte Carlo setting from \citet{giles2020approximate}, whereas our earlier bounds from section~\ref{sec:approximate_gaussian_random_variables} are. 



\subsection{Expected time savings}

Introducing approximate random variables within the nested multilevel Monte Carlo framework offers considerable time savings, which we are able to estimate. Introducing the regular multilevel estimator $ \hat{\theta} $ and the nested multilevel estimator $ \tilde{\theta} $ where
\begin{equation*}
\hat{\theta}  \coloneqq \sum_{l=0}^{L} \dfrac{1}{\hat{m}_l} \sum^{\hat{m}_l} \hat{P}_l - \hat{P}_{l-1}
\qquad \text{and} \qquad
\tilde{\theta} \coloneqq \sum_{l=0}^L \dfrac{1}{\tilde{m}_l} \sum^{\tilde{m}_l} \tilde{P}_l - \tilde{P}_{l-1} + \dfrac{1}{\tilde{M}_l} \sum^{\tilde{M}_l} \hat{P}_l - \hat{P}_{l-1} - \tilde{P}_l + \tilde{P}_{l-1},
\end{equation*}
then $ \hat{m}_l $, $ \tilde{m}_l $, and $ \tilde{M}_l $ paths generated, each with a computational time cost of $ \hat{c}_l $, $ \tilde{c}_l $, and $ \tilde{C}_l $, and variance $ \hat{v}_l $, $ \tilde{v}_l $, and $ \tilde{V}_l $ respectively. 

Each of these estimators will have an error due to the finite number of paths used and the approximation scheme used. The error arising from these two factors is commonly referred to as the variance-bias trade off, where the mean squared error (MSE) of an estimator $ \theta \in \{\hat{\theta}, \tilde{\theta}\} $ is given by $ \text{MSE}(\theta) = \mathbb{V}(\theta) + \text{Bias}^2(\theta)$ \citep[page~16]{glasserman2013monte}. Setting the desired MSE to $ \varepsilon^2 $ and choosing the maximum simulation fidelity such that we just satisfy $ \text{Bias}^2(\theta) \leq \tfrac{\varepsilon^2}{2} $, then we can derive an expression for the total computational time $ T $. Forcing $ T $ to be minimal is achieved by performing a constrained minimisation with an objective function $ \mathscr{F} $. Considering the estimator $ \hat{\theta} $, the corresponding objective function is $ \hat{\mathscr{F}} \coloneqq \sum_{l=0}^{L} \hat{m}_l \hat{c}_l + \mu (\sum_{l=0}^{L} \tfrac{\hat{v}_l}{\hat{m}_l} - \tfrac{\varepsilon^2}{2}) $, where $ \mu $ is a Lagrange multiplier enforcing the constraint $ \mathbb{V}(\hat{\theta}) = \tfrac{\varepsilon^2}{2} $. Treating the number of paths as a continuous variable this is readily minimised to give
\begin{equation*}
\hat{T} = 2\varepsilon^{-2}\left(\sum_{l=0}^L \sqrt{\hat{v}_l \hat{c}_l}\right)^2 
\qquad \text{and} \qquad 
\tilde{T} = 2\varepsilon^{-2} \left(\sum_{l=0}^L \sqrt{\tilde{v}_l \tilde{c}_l} + \sqrt{\tilde{V}_l \tilde{C}_l}\right)^2,
\end{equation*}
where the minimal number of paths required given by
\begin{equation*}
\hat{m}_l = \varepsilon^{-1} \sqrt{\dfrac{2\hat{T}\hat{v}_l}{\hat{c}_l}},
\qquad 
\tilde{m}_l  = \varepsilon^{-1} \sqrt{\dfrac{2\tilde{T} \tilde{v}_l}{\tilde{c}_l}}, 
\qquad 
\text{and}
\qquad 
\tilde{M}_l  = \varepsilon^{-1} \sqrt{\dfrac{2\tilde{T} \tilde{V}_l}{\tilde{C}_l}},
\end{equation*}
and hence an overall saving of
\begin{equation*}
\tilde{T} 
\approx 2\varepsilon^{-2} \left(\sum_{l=0}^L \sqrt{\hat{v}_l \hat{c}_l} \left( \sqrt{\dfrac{\tilde{v}_l\tilde{c}_l}{\hat{v}_l\hat{c}_l}} + \sqrt{\dfrac{\tilde{V}_l \tilde{C}_l}{\hat{v}_l \hat{c}_l}}\right)\right)^2 
\leq \hat{T} \max_{l \leq L} \left\{ \dfrac{\tilde{v}_l\tilde{c}_l}{\hat{v}_l\hat{c}_l} \left(1 + \sqrt{\dfrac{\tilde{V}_l \tilde{C}_l}{\tilde{v}_l \tilde{c}_l}}\right)^2\right\}.
\end{equation*}
For modest fidelity approximations, we will see that $ \tilde{v}_l \approx \hat{v}_l $, in which case the term $ \tfrac{\tilde{v}_l\tilde{c}_l}{\hat{v}_l\hat{c}_l} \approx \tfrac{\tilde{c}_l}{\hat{c}_l}$ measures the potential time savings, and the term $ (1 + (\tilde{V}_l \tilde{C}_l / \tilde{v}_l \tilde{c}_l)^{1/2})^2 $ assesses the efficiency of realising these savings. We can directly see a balance needs to be achieved. The approximations should be sufficiently fast so that there is the potential for large and appreciable savings. However, the approximations need to be a sufficiently high fidelity such that the variance of the expensive four way difference is sufficiently lower than the variance of the cheaper two way difference.

Although we have usable estimates for the costs of each estimator based on the results from table~\ref{tab:implementation_times}, the variance will depend on the stochastic process being simulated, the simulation method being used, and the approximation employed. To make our estimate more concrete and quantify possible variance reductions, let us consider a geometric Brownian motion where $ a(t, X_t) \coloneqq \mu X_t $ and $ b(t, X_t) \coloneqq \sigma X_t $ for two strictly positive constants $ \mu $ and $ \sigma $. In our simulations we will take $ \mu = 0.05 $, $ \sigma = 0.2 $, $ X_0 = 1 $, and $ T = 1 $ \citep[6.1]{giles2008multilevel}. Additionally, we will form the coarse level's Weiner increments by pair-wise summing the fine level's (which uses the fine time increment $ \delta^{\mathrm{f}} $). Importantly, the uniform random variable samples producing the exact Gaussian random variables will be the same as those used for producing the approximate Gaussian random variables, ensuring a tight coupling. Thus for some approximation $ \tilde{\Phi}^{-1} \approx \Phi^{-1} $, such as those from section~\ref{sec:approximate_gaussian_random_variables}, for a single uniform sample $ U_n \sim \mathcal{U}(0, 1) $ then $ Z_n \coloneqq \Phi^{-1}(U_n) $ and $ \tilde{Z}_n \coloneqq \tilde{\Phi}^{-1}(U_n) $, where the $ U_n $ is the same for both of these.

The variance for the various multilevel terms for different time increments for the Euler-Maruyama and Milstein schemes are shown in figure~\ref{fig:variance_reduction} for various approximations. We consider the underlying process itself, corresponding to the functional $ P(X) = X $. The piecewise constant approximation uses 1024 intervals, which from figure~\ref{fig:piecewise_constant_gaussian_approximation_error} has a RMSE of approximately $ 10^{-2} $. The piecewise linear approximation uses 15 dyadic intervals (16 coefficients stored to correctly handle $ \tfrac{1}{2} $), which from figure~\ref{fig:piecewise_linear_gaussian_approximation_error} has a similar RMSE of approximately $ 10^{-2} $. The piecewise cubic approximation uses the same number of intervals, which from figure~\ref{fig:piecewise_linear_gaussian_approximation_error}, its RMSE is approximately $ 10^{-4} $.

\begin{figure}[htb]
\centering

\hfill
\subfigure[The Euler-Maruyama scheme.\label{fig:variance_reduction_euler_maruyama_scheme}]{\includegraphics{variance_reduction_euler_maruyama_scheme}}\hfill 
\subfigure[The Milstein scheme.\label{fig:variance_reduction_milstein_scheme}]{\includegraphics{variance_reduction_milstein_scheme}}\hfill

\caption{The variances from using the Euler-Maruyama and Milstein schemes for a geometric Brownian motion for the functional $ P(X) = X $. The ($ \blacklozenge $)-marker is the two way difference $ \hat{P}^{\mathrm{f}} - \hat{P}^{\mathrm{c}} $, and the remaining markers (\raisebox{-0.1em}{\huge$ \bullet$}, {\large $ \blacktriangledown $}, $ \blacksquare $) are the four way difference $ \hat{P}^{\mathrm{f}} - \hat{P}^{\mathrm{c}} - \tilde{P}^{\mathrm{f}} + \tilde{P}^{\mathrm{c}} $ for the various approximations. (\raisebox{-0.1em}{\huge$ \bullet$}) Piecewise constant. ({\large $ \blacktriangledown $}) Piecewise linear. ($ \blacksquare $) Piecewise cubic.}
\label{fig:variance_reduction}

\Description[The multilevel variance of the Euler-Maruyama and Milstein schemes.]{The variance of the underlying process and the multilevel corrections decreases with finer discretisation with the anticipated strong convergence orders. The piecewise constant and linear functions have a consistently smaller variance than the underlying process, typically being about 5 orders of magnitude less, and the piecewise cubic a further 5 less.}
\end{figure}

We can see from figure~\ref{fig:variance_reduction} that the two way differences exhibit the usual strong convergence order $ \tfrac{1}{2} $ and 1 of the Euler-Maruyama and Milstein schemes, as expected \citep{kloeden1999numerical}. Furthermore, as the functional is differentiable and Lipschitz continuous \citep{giles2020approximate,sheridan2020nested}, this strong convergence rate is preserved for the four way differences. \citeauthor{giles2020approximate} \citep{giles2020approximate,sheridan2020nested} derive this result for the Euler-Maruyama scheme, but that it is also observed for the Milstein scheme remains open to analysis. Aside from this, the key point to note from figure~\ref{fig:variance_reduction} is the substantial drop in the variance between the two way and four way variances. This drop in the variance is large for the piecewise constant and linear approximations, and huge for the piecewise cubic approximation. 

Estimating the best cost savings seen compared to Intel (HA) from table~\ref{tab:implementation_times}, the variance reductions from figure~\ref{fig:variance_reduction_euler_maruyama_scheme}, and using the simplifications $ \tilde{C}_l \approx \hat{c}_l + \tilde{c}_l $ and $ \tilde{v}_l \approx  \hat{v}_l $, then the estimated speed ups and their efficiencies are shown in table~\ref{tab:savings}. We can see from this that the piecewise linear approximation would give the largest savings, although the savings from the piecewise constant and cubic approximations are quite similar. Notice that while the piecewise cubic is achieving near perfect efficiency, its cost savings are not substantial to beat the marginally less efficient piecewise linear approximation which offers the larger potential savings. For all the approximations we can that four way difference simulations are required very infrequently. 

\begin{table}[htb]
\caption{The cost savings, variance reductions, and possible speed ups from the various approximations.}
\label{tab:savings}
\renewcommand{\arraystretch}{1.4}  % This is only local to this one table.
\begin{tabular}{lcccccc}
Approximation  & Cost & Variance & Speed up & Efficiency (\%) & $ \tfrac{\tilde{m}_l}{\hat{m}_l} $ & $ \tfrac{\tilde{m}_l}{\tilde{M}_l} $ \\[0.3em]
\hline
Piecewise constant & $ \tilde{c}_l \approx \tfrac{1}{6}\hat{c}_l $ & $ \tilde{V}_l \approx 2^{-13}\hat{v}_l $ & 5.7 & 94.4 & 1.03 & 240 \\
Piecewise linear  & $ \tilde{c}_l \approx \tfrac{1}{7}\hat{c}_l $& $ \tilde{V}_l \approx 2^{-14}\hat{v}_l $& 6.7 & 95.7 & 1.02 & 360 \\
Piecewise cubic  & $ \tilde{c}_l \approx \tfrac{1}{5}\hat{c}_l $& $ \tilde{V}_l \approx 2^{-25}\hat{v}_l $& 5.0 & 99.9 & 1.00 & 14000\\ 
\end{tabular}
\end{table}

These cost savings are idealised in respect that we have attributed the cost entirely to the generation of the random numbers. While this is quite a common assumption, the validity of this assumption will diminish the faster the approximations become, as the basic cost of the other arithmetic operations becomes significant. Thus, while a practitioner should have their ambitions set to achieve these savings, they should set more modest expectations.

\section{The non central \texorpdfstring{$ \bm{\chi^2} $}{chi-squared} distribution}
\label{sec:the_non_central_chi_squared_distribution}

A second distribution of considerable practical interest is the non central $ \chi^2 $ distribution, which regularly arises in interest rate models from the Cox-Ingersoll-Ross (CIR) short rate model \citep{cox1985theory}. The distribution is parametrised as $ \chi^2_\nu(\lambda) $, where $ \nu > 0 $ denotes the degrees of freedom and $ \lambda \geq 0 $ the non centrality parameter, where we denote the inverse cumulative distribution function as $ C^{-1}_\nu(\cdot; \lambda) $. Having such a parametrised distribution will naturally increase the complexity of any implementation, whether it be exact or approximate. This added complexity makes the distribution considerably more expensive to compute. 

To gauge the procedure used, the associated function in Python's SciPy package (\texttt{ncx2.ppf}) calls the C routine \texttt{CDFCHN} from the CDFLIB library from \citet{brown1994dcdflib} (implementations available \citep{burkardt2020cdflib}). This will compute the value by a root finding routine \citep[algorithm~R]{bus1975two} on the offset cumulative distribution function $ C_\nu(\cdot;\lambda) $, where $ C_\nu(\cdot;\lambda) $ is itself computed by a complicated series expansion \citep[(26.4.25)]{abramowitz1948handbook} involving the cumulative distribution function for the central $ chi^2 $ distribution. Overall, there are very many stages involved, and as remarked by \citet[\texttt{cdflib.c}]{burkardt2020cdflib}: \textit{``Very large values of
[$ \lambda $] can consume immense computer resources''}. The analogous function \texttt{ncx2inv} in MATLAB from its statistics and machine learning toolbox appears to follow a similar approach based on its description \citep[page~4301]{matlab2018statistics}. To indicate the costs, on a local machine, the ratio between sampling from the non central $ \chi^2 $ distribution and the Gaussian distribution (\texttt{norm.ppf} and \texttt{norminv} in Python and MATLAB respectively) are shown, from which it is clear that the non central $ \chi^2 $ distribution can be vastly more expensive than the Gaussian distribution. 

\begin{table}[htb]
\centering    
\caption{The computing ratio between the non-central $ \chi^2 $ and Gaussian distributions.}
\label{tab:non_central_chi_2_times}

\hfill
\subfigure[Python.\label{tab:non_central_chi_2_times_python}]{
\begin{tabular}{|r|rrrrr|}
\multicolumn{1}{c}{\multirow{2}{*}{$ \lambda $}} & \multicolumn{5}{c}{$ \nu $} \\
\cline{2-6}
\multicolumn{1}{c|}{} & 1 &   5  &  10 &  50  & 100 \\
\hline
1    &  37 &  36 &  40 &  54 &  73\\
5    &  40 &  46 &  48 &  62 &  85\\
10   &  54 &  56 &  63 &  69 &  97\\
50   & 101 & 103 & 103 & 144 & 143\\
100  & 191 & 190 & 192 & 189 & 185\\
200  & 243 & 246 & 240 & 233 & 221\\
500  & 465 & 474 & 465 & 446 & 416\\
1000 & 459 & 458 & 455 & 471 & 474 \\
\hline
\end{tabular}
}\hfill
\subfigure[MATLAB.\label{tab:non_central_chi_2_times_matlab}]{
\begin{tabular}{|r|rrrrr|}
\multicolumn{1}{c}{\multirow{2}{*}{$ \lambda $}} & \multicolumn{5}{c}{$ \nu $} \\
\cline{2-6}
\multicolumn{1}{c|}{} & 1 &   5  &  10 &  50  & 100 \\
\hline
1& 168 & 214 & 259 & 456 & 294  \\ 
5& 651 & 782 & 840 & 1510 & 2046  \\ 
10& 935 & 1086 & 1050 & 1838 & 2496  \\ 
50& 3000 & 2969 & 2562 & 4118 & 5333  \\ 
100& 4929 & 3461 & 5039 & 6046 & 6299  \\ 
200& 9456 & 9603 & 10129 & 11524 & 12766  \\ 
500& 22691 & 22713 & 22702 & 23328 & 26273  \\ 
1000& 45872 & 43968 & 43807 & 44563 & 46780  \\
\hline
\end{tabular}
}\hfill
\end{table}

\subsection{Approximating the non central $ \chi^2 $ distribution}

Fo approximating the non central $ \chi^2 $ distribution, we simplify our considerations by taking $ \nu $ to be fixed, and thus the distribution then only has the parameter $ \lambda $ varying. While this may appear a gross simplification, we will see that for the a numerical scheme using a fixed size time step $ \delta $, then $ \nu $ is a constant, and thus this simplification is appropriate.  

We define the function $ P_x(\cdot;y) $ for $ x > 0 $ and $ 0 < y < 1  $ as 
\begin{equation*}
P_x(U;y) \coloneqq \sqrt{\dfrac{x}{4y}} \left( \dfrac{y}{x}  C^{-1}_{x}\left(U; \dfrac{(1 - y)x}{y}\right) - 1\right)
\qquad \text{so that} \qquad 
C^{-1}_{\nu}(U;\lambda) = \lambda + \nu + 2 \sqrt{\lambda + \nu} P_\nu\left(U;\dfrac{\nu}{\lambda + \nu}\right).
\end{equation*}
This is because $ P_x(\cdot;y) $ is better scaled than $ C^{-1}_{\nu}(\cdot;\lambda) $ for the range of possible parameters, with the limits
\begin{equation*}
P_x(U;y) \xrightarrow{y\to 0} \Phi^{-1}(U) 
\qquad \text{and} \qquad 
P_x(U;y) \xrightarrow{y\to 1} \dfrac{C^{-1}_x(U)}{2\sqrt{x}} - \dfrac{\sqrt{x}}{2},
\end{equation*}
where $ C^{-1}_\nu(\cdot) $, without the parameter $ \lambda $, is the inverse cumulative distribution function of the central $ \chi^2 $ distribution. Thus to construct our approximation $ \tilde{C}^{-1}_{\nu}(\cdot;\lambda) \approx C^{-1}_{\nu}(\cdot;\lambda) $, we first construct a piecewise polynomial approximation of  $ \tilde{P}_x(\cdot;y) \approx P_x(\cdot;y) $, and then define 
$ \tilde{C}^{-1}_{\nu}(\cdot;\lambda) = \lambda + \nu + 2 \sqrt{\lambda + \nu} \tilde{P}_\nu(\cdot;\tfrac{\nu}{\lambda + \nu}) $. An example piecewise linear approximation $  \tilde{C}^{-1}_{\nu}(\cdot;\lambda) $ is shown in figure~\ref{fig:non_central_chi_squared_linear_approximation} for various values of the non centrality parameter, using 8 intervals and 16 interpolation values. We can see from figure~\ref{fig:non_central_chi_squared_linear_approximation} that the fidelity of the approximation appears quite high across a range of parameter values. 

\begin{figure}[htb]
\centering

\hfill
\subfigure[Example piecewise linear approximations to the non central $ \chi^2 $ distribution with $ \nu = 1 $ and $ \lambda \in \{1,10,20\} $ using 8 intervals and 16 interpolation values.\label{fig:non_central_chi_squared_linear_approximation}]{\includegraphics{non_central_chi_squared_linear_approximation}}\hfill
\subfigure[The variance of the terms from simulating the CIR process. The (\raisebox{-0.1em}{\huge$ \bullet$})-marker is the underlying CIR process, and the markers ({\large $ \blacktriangledown $}, $ \blacksquare $, $ \blacklozenge $) indicate the variances of possible two way multilevel corrections. ({\large $ \blacktriangledown $}) Euler-Maruyama. ($ \blacksquare $) Piecewise linear. ($ \blacklozenge $) Piecewise cubic.\label{fig:cir_process_variance_reduction}]{\includegraphics{variance_reduction_cir_process}}\hfill 
\caption{The non central $ \chi^2 $ distribution from the CIR process, and the variance of path simulations and possible multilevel corrections.}
\label{fig:cir_process}

\Description[Piecewise linear approximation to the non central chi-squared distribution, and the multilevel variances.]{The piecewise linear approximation to the non central chi-squared distribution with 8 intervals achieved a very good fidelity for a range of non centrality parameter values. The variance of the underlying process is a constant which is independent of the discretisation, and so are the variances of the multilevel corrections for piecewise linear and cubic approximations. The variance of a multilevel Euler-Maruyama correction decreases with a strong order of convergence of 0.5. Both approximations have much lower variances than the Euler-Maruyama approximation, with the linear have a variance reduction by approximately 5 orders of, and the piecewise cubic a further 3.}
\end{figure}

There are only two difficulties with constructing such an approximation. The first is that the distribution is no longer rotationally symmetry, which is easily remedied by constructing two approximations for the domains $ (0, \tfrac{1}{2}] $ and $ (\tfrac{1}{2}, 1) $. The second difficulty dealing with the parametrisation. Noting that $ y \in (0, 1) $ will be associated with the non centrality parameter, we construct several approximations for various values of $ y $ (knowing the limiting cases) and linear interpolate the appropriate values. A good choice of knot points are equally spaced values of $ \sqrt{y} $.


%RMSE for various parameter ranges for piecewise linear and cubic, (maybe quintic too?), with fixed numbers of interpolating functions.
\begin{table}[htb]
\centering    
\caption{The RMSE of approximations to the non-central $ \chi^2 $ distribution.}
\label{tab:non_central_chi_2_rmse}

\hfill
\subfigure[Piecewise linear.\label{tab:non_central_chi_2_rmse_linear}]{
\begin{tabular}{|r|ccccc|}
\multicolumn{1}{c}{\multirow{2}{*}{$ \lambda $}} & \multicolumn{5}{c}{$ \nu $} \\
\cline{2-6}
\multicolumn{1}{c|}{} & 1 &   5  &  10 &  50  & 100 \\
\hline
1    &  0.036 & 0.036 & 0.041 & 0.070 & 0.095 \\
5    &  0.045 & 0.047 & 0.050 & 0.076 & 0.100 \\
10   &  0.054 & 0.056 & 0.059 & 0.081 & 0.104 \\
50   &  0.098 & 0.099 & 0.101 & 0.116 & 0.133 \\
100  &  0.134 & 0.135 & 0.136 & 0.148 & 0.161 \\
200  &  0.186 & 0.187 & 0.188 & 0.196 & 0.207 \\
\hline
\end{tabular}
}\hfill
\subfigure[Piecewise cubic.\label{tab:non_central_chi_2_rmse_cubic}]{
\begin{tabular}{|r|ccccc|}
\multicolumn{1}{c}{\multirow{2}{*}{$ \lambda $}} & \multicolumn{5}{c}{$ \nu $} \\
\cline{2-6}
\multicolumn{1}{c|}{} & 1 &   5  &  10 &  50  & 100 \\
\hline
1  &    0.004 & 0.005 & 0.006 & 0.007 & 0.006 \\
5  &     0.004 & 0.004 & 0.005 & 0.010 & 0.015 \\
10  &   0.006 & 0.005 & 0.005 & 0.009 & 0.014 \\
50  &    0.006 & 0.007 & 0.005 & 0.010 & 0.011 \\
100 &   0.013 & 0.008 & 0.009 & 0.011 & 0.014 \\
200 &   0.009 & 0.012 & 0.011 & 0.012 & 0.015 \\
\hline
\end{tabular}
}\hfill
\end{table}

\subsection{Simulating the Cox-Ingersoll-Ross process}

We have mentioned that the non central $ \chi^2 $ distribution arises from the Cox-Ingersoll-Ross (CIR) process, which can be written as $ \dd{X_t} = \kappa (\theta - X_t) \dd{t} + \sigma \sqrt{X_t} \dd{W_t} $ for strictly positive parameters $ \kappa $, $ \theta $ and $ \sigma $. The distribution for $ X_T $ conditioned on $ X_0 $ is a non central $ \chi^2 $ distribution with $ \nu = \tfrac{4\kappa\theta}{\sigma^2} $ and $ \lambda = \tfrac{4\kappa X_0 \exp(-\kappa T)}{\sigma^2(1 - \exp(-\kappa T))} $. Simulating this with the Euler-Maruyama scheme forcibly approximates the non central $ \chi^2 $ distribution as a Gaussian distribution, and can require very finely resolved path simulations to bring the bias down to an acceptable level, as explored by \citet{broadie2006exact}. Similarly the Euler-Maruyama scheme is numerically ill posed due to the $ \sqrt{X_t} $ term, and several adaptions exist to handle this appropriately \citep{deelstra1998convergence,lord2010comparison,berkaoui2008euler,higham2002strong,alfonsi2005discretization,alfonsi2008second,alfonsi2010high,dereich2012euler,cozma2020strong_euler,gyongy2011note}, especially when the Feller condition $ 2\kappa\theta \geq \sigma^2 $ is not satisfied \citep{feller1951two,gyongy1998note}.

Rather than approximating the non central $ \chi^2 $ with an exact Gaussian when using the Euler-Maruyama scheme, we propose using an approximate non central $ \chi^2 $ random variable, such as from a piecewise linear approximation. This has the benefit of offering vast time savings, whilst introducing far less bias than the Euler-Maruyama scheme. We know from our earlier results from table~\ref{tab:implementation_times} that a piecewise linear approximation should only average at worst only a few clock cycles, giving vast savings from the times seen in table~\ref{tab:non_central_chi_2_times}, and the approximations appear to achieve a high fidelity, as was seen from table~\ref{tab:non_central_chi_2_rmse}.

We can compare path simulations of the CIR process using exact non-central $ \chi^2 $ distributions, piecewise linear and cubic approximations, and simulation using the truncated Euler-Maruyama scheme from \citet{higham2002strong} (where $ \sqrt{X_n} \to \sqrt{\lvert X_n \rvert} $). We set $ \kappa= \tfrac{1}{2}$ and  $ \theta = \sigma =T = X_0 = 1 $ (satisfy the Feller condition. The variance of underlying process and the difference between this and the approximations are shown in figure~\ref{fig:cir_process_variance_reduction}. 

From figure~\ref{fig:cir_process_variance_reduction} we can see the exact underlying process's variance does not vary with the discretisation, which is to be expected as the simulation is from the exact distribution, and thus has no bias. We can see that the Euler-Maruyama approximation to the process exhibits a strong convergence order $ \tfrac{1}{2} $, as expected \citep{higham2002strong,gyongy1998note}. For the piecewise linear function, as the fidelity of the approximation will not vary with the discretisation, then subsequently the variance also does not vary. Furthermore, given the high fidelity seen in table~\ref{tab:non_central_chi_2_rmse_linear}, the drop in the variance is approximately $ 2^{-15} $. Similarly, the even higher fidelity cubic approximation demonstrates the same behaviour, but with a drop in variance of $ 2^{-25} $. With such large drops in the variance, we would expect our approximations to achieve near perfect efficiencies with respect to recovering the full time savings offered from our approximations, which the user can expect to be considerably more substantial than those for the Gaussian distribution. 

\section{Conclusions}
\label{sec:conclusions}

The expense of sampling random variables is a widespread challenge, common across a range of disciplines including: encryption, animation \citep{lee2017vectorized}, and financial simulations. In the work presented, we proposed, developed, analysed, and implemented approximate random variables as a means of circumventing this problem for a variety of modern hardware. By incorporating these into a nested multilevel Monte Carlo framework we showcased how the full speed improvements offered can be recovered with near perfect efficiency without losing any accuracy. With a detailed treatment, we showed that even for basic simulations of geometric Brownian motions requiring Gaussian random variables speed ups of a factor 5 or more can be expected. The same framework was also demonstrated to be applicable to the more difficult Cox-Ingersoll-Ross process and its non central $ \chi^2 $ distribution, offering the potential for vast computational savings. 

For sampling from a wide class of distributions, we inspected the inverse transform method. Unfortunately, this method is expensive as it relies on a distribution's inverse cumulative distribution function. We showcased that many implementations of the Gaussian distribution's inverse cumulative distribution function, while accurate to near machine precision, are ill suited in several respects for modern vectorised hardware. To address this problem, we introduced a generalised notion of approximate random variables, produced from using approximations to a distribution's inverse cumulative distribution function. The two major classes of approximation we introduced were: piecewise constant approximations using equally spaced intervals, and piecewise polynomial approximations using geometrically small intervals dense near the distribution's tails. These cover a wide class of possible approximations, and notably recover as special cases: Rademacher random variables and the weak Euler-Maruyama scheme \citep{glasserman2013monte}, moment matching schemes by \citet{muller2015improving}, and truncated bit approximations by \citet{giles2019random_quadrature}. For piecewise constant and linear approximations, we analysed and bound the error for a range of possible norms. The significance of these bounds is that they are valid for arbitrarily high moments, which extends the results from \citet{giles2019random_quadrature}, and crucially is necessary for the nested multilevel Monte Carlo analysis by \citet{giles2020approximate}. Lastly, these approximations were able to achieve very high fidelities, with the piecewise polynomials using geometric intervals have an inherently very high fidelity. 

With the approximations detailed from a mathematical perspective, we highlighted two possible implementations in C \citep{sheridan2020approximate_random}. The benefit of these approximations is that they are by design ideally suited for modern vector hardware and achieve the highest possible computational speeds. They can be readily vectorised using OpenMP SIMD directives, have no conditional branching, avoid division and expensive function evaluations, and only require simple additions, multiplications, and bit manipulations. The piecewise constant and linear implementations were orders of magnitude faster than most freely available open-source libraries, and typically a factor of 5--7 times faster than the proprietary Intel MKL library, achieving close to the maximum speed of just reading and writing to memory. This speed comes from the simplicity of their operations, and heavy capitalising on the fast speeds of querying the cache and vector registers. 

Incorporating approximate random variables into the nested multilevel Monte Carlo framework by \citet{giles2020approximate}, the low errors and fast speeds of the approximations could be exploited to obtain full speed benefits without losing accuracy. Inspecting the appropriate multilevel variance reductions, we demonstrated how practitioners can expect to obtain speed improvements of a factor of 5--7, where the fastest approximation was the piecewise linear approximation. This appears to be the case when using either the Euler-Maruyama or Milstein schemes. 

Considering the Cox-Ingersoll-Ross process \citep{cox1985theory}, this is known to give rise to the non central $ \chi^2 $ distribution, which is an example of a parametrised distribution, and is also colossally expensive to sample from, as we demonstrated. We applied our approximation framework to this to produce a parametrised approximation. The error of using our approximate random variables was orders of magnitude lower than approximating paths using the Euler-Maruyama scheme, showing our approximate random variables are considerably more suitable for generating path simulations than the Euler-Maruyama scheme. This resolves the problem of the Euler-Maruyama scheme having a very large bias for the Cox-Ingersoll-Ross process \citep{broadie2006exact}. Furthermore, as our implementation was only a marginal adaptation of the Gaussian distribution's implementation, the speed improvement offered are vast. 

All of the code to produce these figures, tables, and approximations is freely available and hosted in repositories by \citet{sheridan2020approximate_inverse,sheridan2020approximate_random}, along with a getting started guide aimed at practitioners \citep{sheridan2020approximate_random}.

\section{Acknowledgements}

We would like to acknowledge and thank those who have financially sponsored this work. This includes the Engineering and Physical Sciences Research Council (EPSRC) and Oxford University's centre for doctoral training in Industrially Focused Mathematical Modelling (InFoMM), with the EP/L015803/1 funding grant. Furthermore, this research stems from a PhD project \citep{sheridan2020nested} which was funded by Arm and NAG. Additionally, funding was also provided by the Inference, Computation and Numerics for Insights into Cities (ICONIC) project, and the programme grant EP/P020720/1. Lastly, Mansfield College Oxford also contributed funds.  

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}